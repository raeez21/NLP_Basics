{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2176b624-2c43-467f-b1b2-bafc9fa48bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238ac2b-fb10-434a-ba85-71f1fdabdeed",
   "metadata": {},
   "source": [
    "model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7327b9ea-d7a3-45dc-a225-671ae95b1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperpara\n",
    "seq_len = 8\n",
    "\n",
    "#model hyperpara\n",
    "embed_dim = 128\n",
    "\n",
    "# training hyperpara\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365e928-fa8f-450b-9ecd-e7c395e9e896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d49f5e7-fbdf-44a3-84d5-b74031f306ff",
   "metadata": {},
   "source": [
    "One attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fea5450-678d-48f4-aef3-dff972ea5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one attention head\n",
    "class OneAttentionHead(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # create q,k,v matrices\n",
    "        self.key = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.query = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.W0 = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #run the token embedd vectors through attention\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        y = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n",
    "        #is_causal make sures the time causal mask is included in calculations\n",
    "        y = self.W0(y) #linear transfor\n",
    "\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adb37972-4b74-460b-a48a-371630433798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneAttentionHead(\n",
      "  (key): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (query): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (value): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (W0): Linear(in_features=128, out_features=128, bias=False)\n",
      ")\n",
      "\n",
      "Output (torch.Size([5, 8, 128])): \n",
      "tensor([[[-9.4601e-02,  4.4920e-01, -1.6067e-01,  ..., -2.6583e-01,\n",
      "          -2.5213e-01, -8.2024e-02],\n",
      "         [-1.8009e-01,  2.7770e-01, -3.0519e-01,  ..., -3.2870e-01,\n",
      "          -1.7446e-01, -2.4908e-01],\n",
      "         [-7.4834e-02,  2.3881e-01, -1.7252e-01,  ...,  3.9444e-02,\n",
      "          -2.8884e-02, -1.0802e-01],\n",
      "         ...,\n",
      "         [-8.4594e-02,  3.7004e-02, -1.5335e-01,  ..., -1.3751e-01,\n",
      "           5.5471e-02, -1.3126e-01],\n",
      "         [-9.1445e-02,  5.3801e-02, -2.9243e-01,  ..., -1.8555e-01,\n",
      "          -8.3495e-03, -2.5617e-01],\n",
      "         [-4.2297e-02,  8.8682e-02, -2.2556e-01,  ..., -1.3645e-01,\n",
      "          -5.6437e-02, -2.4334e-01]],\n",
      "\n",
      "        [[-1.6051e-01, -7.5624e-02, -6.3366e-01,  ...,  7.0565e-01,\n",
      "          -1.0151e-01, -1.2307e-01],\n",
      "         [-2.6951e-01, -7.0397e-02, -2.4772e-01,  ...,  4.4679e-01,\n",
      "          -3.6482e-02, -4.3941e-01],\n",
      "         [-2.6120e-01, -5.4447e-02, -3.5077e-01,  ...,  4.4395e-01,\n",
      "          -8.0739e-02, -2.3561e-01],\n",
      "         ...,\n",
      "         [-1.4234e-01, -1.4809e-02, -2.3670e-01,  ...,  2.9240e-01,\n",
      "          -1.6871e-02, -2.4080e-01],\n",
      "         [-7.9776e-02,  2.7531e-02, -1.2120e-01,  ...,  1.8318e-01,\n",
      "           7.3075e-02, -1.1113e-01],\n",
      "         [-5.5135e-02, -3.2552e-02, -1.0108e-01,  ...,  2.1294e-01,\n",
      "           8.2501e-02, -1.4088e-01]],\n",
      "\n",
      "        [[-1.5182e-01, -5.8527e-01, -9.1849e-02,  ...,  4.2130e-01,\n",
      "          -3.8945e-01, -3.6822e-01],\n",
      "         [-5.4522e-02,  1.9988e-02, -4.2326e-01,  ..., -7.6703e-02,\n",
      "          -3.8529e-01, -2.9310e-01],\n",
      "         [-7.3172e-02, -1.4498e-01, -2.6523e-01,  ...,  7.6952e-02,\n",
      "          -3.5658e-01, -2.5312e-01],\n",
      "         ...,\n",
      "         [-7.6274e-03, -8.9831e-02, -9.9732e-02,  ..., -8.9480e-02,\n",
      "           9.7213e-03, -1.4084e-01],\n",
      "         [ 2.4353e-02,  2.8760e-02, -2.2198e-02,  ..., -1.9037e-01,\n",
      "           6.4214e-02, -2.5828e-02],\n",
      "         [-7.9517e-03,  3.1362e-02, -9.3815e-02,  ..., -2.0149e-01,\n",
      "           7.6825e-02, -3.6806e-02]],\n",
      "\n",
      "        [[-1.1445e-01,  5.6028e-01,  6.9188e-02,  ...,  2.5216e-01,\n",
      "          -1.3195e-02, -4.3178e-01],\n",
      "         [-1.4445e-01,  3.3179e-01, -1.6280e-01,  ..., -4.0813e-02,\n",
      "          -1.0044e-01, -7.3457e-02],\n",
      "         [-1.3846e-01,  9.8273e-02, -7.8357e-02,  ...,  2.7789e-02,\n",
      "           1.8658e-01, -1.3803e-01],\n",
      "         ...,\n",
      "         [-7.6700e-03, -4.0443e-02, -1.1919e-01,  ..., -2.1008e-01,\n",
      "          -1.0143e-01, -2.2420e-01],\n",
      "         [-3.4885e-02, -3.2413e-02, -1.3553e-01,  ..., -1.9315e-01,\n",
      "          -1.2336e-02, -2.5398e-01],\n",
      "         [ 3.8166e-02, -4.0286e-04, -1.1801e-01,  ..., -1.7538e-01,\n",
      "           5.3743e-02, -2.1276e-01]],\n",
      "\n",
      "        [[ 7.4312e-02,  5.5895e-01,  1.9125e-01,  ..., -1.1970e-01,\n",
      "          -9.6837e-03,  1.2090e-01],\n",
      "         [ 6.1560e-02,  2.0396e-01,  3.4370e-01,  ..., -5.9751e-02,\n",
      "           5.3085e-02, -1.8753e-01],\n",
      "         [-2.4228e-02,  1.8310e-01,  1.8555e-01,  ..., -1.0368e-01,\n",
      "           1.1368e-01, -1.3327e-01],\n",
      "         ...,\n",
      "         [-3.2862e-02, -1.3479e-01, -1.4754e-01,  ..., -3.2990e-02,\n",
      "           1.0157e-01,  8.8558e-02],\n",
      "         [-5.5423e-02, -2.0936e-01, -2.6078e-02,  ..., -1.5659e-01,\n",
      "           4.4965e-02,  2.0456e-02],\n",
      "         [ 3.7629e-02, -1.5665e-01, -1.1867e-01,  ..., -1.3971e-01,\n",
      "           7.2396e-02,  1.2671e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# explore the attention head\n",
    "onehead = OneAttentionHead(embed_dim)\n",
    "\n",
    "print(onehead)\n",
    "\n",
    "#run some fake data through\n",
    "tokenEmebeds = torch.randn(batch_size, seq_len, embed_dim)\n",
    "out = onehead(tokenEmebeds)\n",
    "print(f'\\nOutput ({out.shape}): \\n{out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0524562b-2b64-4706-aebb-582ab3f94e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 128])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenEmebeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b080ee1-4f02-4ccd-b5d8-8b2f4f9e8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In single head attention, we process all of the embeding dims all at once\n",
    "# in multi head, the embed_dimensions broke up into slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e80fb9-f8bb-4519-b3a1-ee33dacbea6d",
   "metadata": {},
   "source": [
    "Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2fe88f4-137c-489c-a895-617379e3bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        #attention sublayer\n",
    "        self.layerNormAttn = nn.LayerNorm(embed_dim)\n",
    "        self.attn = OneAttentionHead(embed_dim)\n",
    "\n",
    "        #feedfwd (MLP) sublayer\n",
    "        self.layerNormMLP = nn.LayerNorm(embed_dim)\n",
    "        self.W1 = nn.Linear(embed_dim,4*embed_dim) # 4x expansion\n",
    "        self.gelu = nn.GELU()\n",
    "        self.W2 = nn.Linear(4*embed_dim, embed_dim) #4x contraction\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        ## ----attention sublayer ------##\n",
    "        # save a copy for pre-attention data\n",
    "        residual = x\n",
    "\n",
    "        # layernorm -> atteniton\n",
    "        h = self.layerNormAttn(x)\n",
    "        attn_out = self.attn(h)\n",
    "        # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        #combine pre attention copy + attention adjustments\n",
    "        x = residual + attn_out\n",
    "\n",
    "        #could do this in one line:\n",
    "        #  x = x + self.attn(self.layerNormAttn(x))\n",
    "        # ------------------------------#\n",
    "\n",
    "        # --------MLP sublayer -------#\n",
    "        # copy of pre_MLP data (output of attn sublayer)\n",
    "\n",
    "        residual2 = x\n",
    "\n",
    "        #layernorm before MLO\n",
    "        h2 = self.layerNormMLP(x)\n",
    "\n",
    "        #expansion-nonlinearity-contraction\n",
    "        mlp_out = self.W2(self.gelu(self.W1(h2)))\n",
    "\n",
    "        #combine pre-MLP copy + MLP adjustment\n",
    "        y = residual2 + mlp_out\n",
    "        #-----------------------------â€“#\n",
    "        # y is [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # y either goes to next transformer block\n",
    "        # or to final unembedding matrix and then to token and text\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48498b7b-c0c1-448e-ad92-01beb954b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): OneAttentionHead(\n",
      "    (key): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (query): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (value): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (W0): Linear(in_features=128, out_features=128, bias=False)\n",
      "  )\n",
      "  (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create and explore an instance\n",
    "transblock = TransformerBlock(embed_dim)\n",
    "\n",
    "print(transblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d31fb73-a37d-4d99-9d10-beaaa76a1787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output (torch.Size([5, 8, 128])): \n",
      "tensor([[[-1.0108, -2.0522,  1.7651,  ..., -1.0905,  0.5553,  0.2084],\n",
      "         [-1.4508,  0.0729,  0.1084,  ...,  0.4905,  0.6473, -2.0556],\n",
      "         [ 1.6218,  0.6352,  0.1972,  ..., -1.1110, -1.0756, -1.0169],\n",
      "         ...,\n",
      "         [-0.3379, -0.2691,  0.7948,  ...,  0.1477, -0.0889, -1.7009],\n",
      "         [-0.9677, -0.0556, -1.2894,  ..., -1.0842, -0.8767, -1.3239],\n",
      "         [ 0.6473,  0.1347, -1.1578,  ..., -0.6961, -1.1902, -0.3936]],\n",
      "\n",
      "        [[-0.3241, -0.0264,  0.6526,  ..., -1.0890,  0.1171, -1.1695],\n",
      "         [ 0.7390,  1.5468,  0.8615,  ..., -0.2498, -0.6122,  0.5031],\n",
      "         [-0.3949,  1.5369, -1.9716,  ..., -0.5512,  1.7584,  0.1625],\n",
      "         ...,\n",
      "         [ 0.9793,  0.5105, -0.3675,  ...,  0.7062, -0.0030,  1.0130],\n",
      "         [ 0.3585, -0.0196, -1.2885,  ...,  0.5898,  0.6689,  0.5115],\n",
      "         [-0.7787, -0.1190, -0.2454,  ...,  0.0558,  0.0030, -0.8798]],\n",
      "\n",
      "        [[-0.0938, -0.8779, -0.5781,  ...,  1.5676, -1.3632,  1.5236],\n",
      "         [ 1.7880,  0.7316,  0.4579,  ..., -0.3944, -1.0531, -0.8995],\n",
      "         [-1.0106, -2.0199, -1.1571,  ..., -1.4945,  1.7682, -0.5330],\n",
      "         ...,\n",
      "         [ 0.8440, -0.2194,  0.8490,  ...,  1.0055, -2.5160, -0.0745],\n",
      "         [-2.0938, -1.7470, -0.3643,  ...,  0.3824, -0.4943, -1.4833],\n",
      "         [-0.3266,  0.5362, -0.5787,  ...,  1.4551, -1.8304,  0.3609]],\n",
      "\n",
      "        [[ 0.6030, -1.3333, -0.1223,  ..., -1.2069, -2.2437,  1.9568],\n",
      "         [-1.0700, -0.2316, -0.5786,  ...,  1.3503,  0.6653, -0.7177],\n",
      "         [ 2.5888, -0.3190, -0.5843,  ...,  1.2157,  0.3627,  0.7053],\n",
      "         ...,\n",
      "         [-0.0402, -2.3403, -0.5077,  ..., -1.0716, -1.2299, -1.4350],\n",
      "         [-0.2674,  0.2008,  0.3382,  ..., -2.3768,  0.5527, -0.1817],\n",
      "         [-1.3150,  2.3883, -1.7877,  ...,  0.1767,  0.1160,  0.0716]],\n",
      "\n",
      "        [[ 2.0518, -0.6839,  0.7478,  ...,  0.3523, -0.9969, -0.7109],\n",
      "         [ 0.4538, -0.7132, -1.1138,  ..., -0.8736, -0.0699, -1.9335],\n",
      "         [-0.7102,  0.2934,  1.1103,  ..., -2.1710, -0.4526,  1.9413],\n",
      "         ...,\n",
      "         [-0.7383,  1.3286, -2.5959,  ...,  0.6040, -0.0637, -1.4560],\n",
      "         [-0.6692,  0.8678, -2.0509,  ...,  0.0900,  1.0505,  0.6852],\n",
      "         [-0.9122,  0.3749,  0.3314,  ..., -1.3849, -1.4198, -0.8406]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#push data through\n",
    "out = transblock(tokenEmebeds)\n",
    "print(f'\\nOutput ({out.shape}): \\n{out}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ee6e5-67e6-44c9-ab16-cf7ab9a81376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
