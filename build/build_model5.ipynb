{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3254918e-c07f-432e-8258-4d66e9d2b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is GPT2 model demo, with multi head attention, multiple transformer\n",
    "# and all other stuff learned so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9265699-ad22-4335-9f03-45c901ec4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# use GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0928843-f9fb-463e-b59b-dfb3b4a1369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper para for GPT2-124M\n",
    "n_vocab = 50257 # GPT2 vocab size\n",
    "embed_dim = 768 #embedding dim\n",
    "seq_len = 1024 #max seq len\n",
    "n_heads = 12 # attention heads\n",
    "n_blocks = 12 # tranformer blocks\n",
    "#each transformer block has 12 atention heads\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d34b9-0f94-47e8-a285-b28fd48bde0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6971a3da-de9e-4cd8-9185-61f2f281b7ac",
   "metadata": {},
   "source": [
    "class for multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2a1905-5b09-478f-9858-78838264464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        #head dimensionality is embed_dim split across the heads\n",
    "        self.num_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "    \n",
    "        # the three Q,K,V weight matrices are init as one, and are split inside attention eqn\n",
    "        self.QKV = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n",
    "    \n",
    "        #final linear projection merges the heads outputs\n",
    "        self.W0 = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # extract the dimension size of the inputs(token embedds)\n",
    "        B, T, E = x.shape # [batch, tokens (or seq_len), embed_dim]\n",
    "        \n",
    "\n",
    "        #push data through Q,K and V in one concatenated matrix\n",
    "        qkv = self.QKV(x) #[batch, seq_len, 3*embed]\n",
    "        q,k,v = torch.split(qkv, E, dim=2) # each matrix is [B,T,E]\n",
    "\n",
    "        # reshape to [B,T,nHeads, head_dim]\n",
    "        # and then transpose to [B, nHeads, T, head_dim]\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2) #[B, num_heads, T, head_dim]\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # Pytorchs SDPA func handles multi head shapes\n",
    "        out = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n",
    "\n",
    "        # recombine heads : (B,nHeads,T,head_dim) -> [B,T,E]\n",
    "        out = out.transpose(1,2).reshape(B,T,E)\n",
    "    \n",
    "\n",
    "        #finallt apply linear mixing matrix\n",
    "        out = self.W0(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8dc859-d167-4e80-b41f-d34e9302b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c63c2385-add1-4d53-b18b-4bf51d36048a",
   "metadata": {},
   "source": [
    "Transfomer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fb8f3d-7e99-4350-8210-e35503b91972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #attention subblock\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim,eps=1e-5)\n",
    "        self.attn = MultiHeadAttention()\n",
    "\n",
    "        #feedfwd (MLP) sublayer\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim,eps=1e-5)\n",
    "        self.mlp_1 = nn.Linear(embed_dim,4*embed_dim,bias=True) # 4x expansion\n",
    "        self.gelu = nn.GELU()\n",
    "        self.mlp_2 = nn.Linear(4*embed_dim, embed_dim, bias=True) #4x contraction\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        ## ----attention sublayer ------##\n",
    "        x_att = self.layernorm_1(x) # pre attn normalisn\n",
    "        x_att = x + self.attn(x_att) # run through attention, then add pre attn activations\n",
    "\n",
    "        #MLP\n",
    "        x_ff = self.layernorm_2(x_att) # pre MLP normlsn\n",
    "        x_ff = x_att + self.mlp_2( self.gelu( self.mlp_1(x_ff)))\n",
    "        \n",
    "        return x_ff\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6860b-a12a-4f10-b853-3c8adf2fe3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570bc14d-87b2-4e7c-a7b1-81e0cca70313",
   "metadata": {},
   "source": [
    "class for full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fd177b-f6af-4b11-a255-ee716825727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full model class, which calls the previously defined classes\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # token + posn embedds\n",
    "        self.wte = nn.Embedding(n_vocab, embed_dim) # token embedds\n",
    "        self.wpe = nn.Embedding(seq_len, embed_dim) # posn embedds\n",
    "\n",
    "        #n mutliple Transformer blocks\n",
    "        # * is a unpacking operator, the list of txf blocks goes into input of Sequential()\n",
    "        self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n",
    "\n",
    "        # embedding to output (linear) layer\n",
    "        self.layernorm_final = nn.LayerNorm(embed_dim,eps=1e-5) # final layernorm after all txf blocks\n",
    "        #unembed matirx\n",
    "        self.final_head = nn.Linear(embed_dim, n_vocab, bias=False)\n",
    "        #final ouput layer (unembedd) tied to token embedd\n",
    "        self.final_head.weight = nn.Parameter(self.wte.weight)\n",
    "\n",
    "    def forward(self, idx):\n",
    "\n",
    "        #----------embeddings-------------##\n",
    "        token_emb = self.wte(idx)  # [B,T,E]   T is seq_len and E is embed_dim\n",
    "        posit_emb = self.wpe(torch.arange(idx.shape[-1],device=device)) #[seq_len, embed_dim]\n",
    "        x = token_emb + posit_emb #[B,T,E]\n",
    "        ##--------------------------------##\n",
    "\n",
    "        #n\n",
    "        ##--pass through each transformer blocks----##\n",
    "        x = self.transformerBlocks(x)\n",
    "        ##-------------------------##\n",
    "\n",
    "        #-----finally unembeddings----##\n",
    "        x = self.layernorm_final(x)\n",
    "        logits = self.final_head(x) # [B,T, n_vocab]\n",
    "        # logits is [batch, seq_len, n_vocab]\n",
    "        return logits\n",
    "\n",
    "    def generate(self,idx,temperature=1.,max_new_tokens=50):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # fwd passb\n",
    "            logits = self(idx[:,-seq_len:]) # [B,T,n_vocab]   get preds, but only from past seq_len tokens \n",
    "            logits = logits[:,-1,:] #[B,n_vocab]   extract last tokens logitsto predict the next\n",
    "\n",
    "            # apply softmax with temp to get prob values over all tokens in vocab - with temp\n",
    "            probs = F.softmax(logits/temperature,dim=-1) #[B,n_vocab]\n",
    "\n",
    "            #probabilistically sample next token from distbn\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # [batch,1]\n",
    "            \n",
    "            #append \n",
    "            idx = torch.cat((idx, idx_next),dim=1) #[batch, (tokens+1)]\n",
    "        return tokx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856caaea-8a82-4ea6-931d-f6694403cde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc7c491-e218-4ca5-b945-22f0ea0981f3",
   "metadata": {},
   "source": [
    "create an instance and test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233ed8af-ecdf-4374-8211-dcd394936b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (transformerBlocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layernorm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (QKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (W0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (mlp_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c25a31-c252-4d6a-b0c7-63ac4b901063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([8, 1024])\n",
      "Output size: torch.Size([8, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "# running some fake data through\n",
    "data = torch.randint(0,n_vocab,size=(batch_size,seq_len)).to(device)\n",
    "out = model(data)\n",
    "print(f'input size: {data.shape}')\n",
    "print(f'Output size: {out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327e3b9-d555-46c8-94af-a755a1689838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b0652e-5363-40e5-8b44-ff5fc4aafbc1",
   "metadata": {},
   "source": [
    "How many parameters do we have??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6c8777-381b-4a0d-b358-c20666060b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "LanguageModel                            [8, 1024]                 [8, 1024, 50257]          --\n",
       "├─Embedding: 1-1                         [8, 1024]                 [8, 1024, 768]            38,597,376\n",
       "├─Embedding: 1-2                         [1024]                    [1024, 768]               786,432\n",
       "├─Sequential: 1-3                        [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    └─TransformerBlock: 2-1             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-1               [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-2      [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-3               [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-4                  [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-5                    [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-6                  [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-2             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-7               [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-8      [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-9               [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-10                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-11                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-12                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-3             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-13              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-14     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-15              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-16                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-17                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-18                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-4             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-19              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-20     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-21              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-22                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-23                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-24                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-5             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-25              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-26     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-27              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-28                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-29                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-30                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-6             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-31              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-32     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-33              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-34                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-35                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-36                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-7             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-37              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-38     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-39              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-40                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-41                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-42                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-8             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-43              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-44     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-45              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-46                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-47                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-48                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-9             [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-49              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-50     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-51              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-52                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-53                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-54                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-10            [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-55              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-56     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-57              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-58                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-59                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-60                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-11            [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-61              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-62     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-63              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-64                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-65                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-66                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "│    └─TransformerBlock: 2-12            [8, 1024, 768]            [8, 1024, 768]            --\n",
       "│    │    └─LayerNorm: 3-67              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─MultiHeadAttention: 3-68     [8, 1024, 768]            [8, 1024, 768]            2,362,368\n",
       "│    │    └─LayerNorm: 3-69              [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "│    │    └─Linear: 3-70                 [8, 1024, 768]            [8, 1024, 3072]           2,362,368\n",
       "│    │    └─GELU: 3-71                   [8, 1024, 3072]           [8, 1024, 3072]           --\n",
       "│    │    └─Linear: 3-72                 [8, 1024, 3072]           [8, 1024, 768]            2,360,064\n",
       "├─LayerNorm: 1-4                         [8, 1024, 768]            [8, 1024, 768]            1,536\n",
       "├─Linear: 1-5                            [8, 1024, 768]            [8, 1024, 50257]          38,597,376\n",
       "===================================================================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.10\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 10044.38\n",
       "Params size (MB): 652.15\n",
       "Estimated Total Size (MB): 10696.59\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# summary of model and params\n",
    "summary(model, input_data=data, col_names =['input_size','output_size', 'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ffe0434-cc7c-46e0-82e4-47acc2955d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we actually dont have 163M params as shown above\n",
    "# This is bcoz summary() doesnot know that unembedd matrix is tied to embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f13b33-62f9-447d-839c-b2735a3a7c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params = 124439808\n"
     ]
    }
   ],
   "source": [
    "print(f'Total trainable params = {163037184 - 38597376}')\n",
    "#124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65217e7-c6e7-46a5-a21d-61ab81ee3c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be723be-a436-448e-935b-de9f9efa02c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ace1e-803c-45e3-a91e-a696e0cb4afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43b1f1-6066-432c-8719-7d1d1f174de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b12353e-1083-4643-8819-a5ed343f676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tempA:\n",
    "    def __init__(self):\n",
    "         self.a = batch_size\n",
    "    def disp(self):\n",
    "        print(\"Val is: \",self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfdc557-f683-4bb1-a5be-9747c8ed182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val is:  8\n"
     ]
    }
   ],
   "source": [
    "a=tempA()\n",
    "a.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28959676-b3f1-4730-9348-6ddd2230cb06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tempA.disp() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tempA.disp() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "a.disp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc322be-6975-4bf7-ac37-d929b3c07edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
