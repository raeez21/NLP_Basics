{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af9e705-3635-43a5-a299-67d9f6a8a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we only have a multi head attention sublayer\n",
    "# not a full transformer block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412438d0-67ae-4fbc-9ad2-8560d6ff4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3d8ce-cf77-428a-ae54-5da7a79e309c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb503521-e8b1-429e-8dd3-dd9b135ea082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparas\n",
    "seq_len = 8 # aka context length\n",
    "\n",
    "\n",
    "#model hyperparas\n",
    "embed_dim = 128\n",
    "n_heads = 4 #n embed_dim/n_heads must be int\n",
    "\n",
    "# training hyperpara\n",
    "batch_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "871695da-04ab-4586-b77e-112b360f6bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello is                        raeez 10\n"
     ]
    }
   ],
   "source": [
    "a=10\n",
    "print(f\"Hello is {'raeez':>28} {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c8408-2503-459c-8adc-1922d6fbfb36",
   "metadata": {},
   "source": [
    "class for multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca7176b-924f-4c30-bdae-fb6adbc6a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        #head dimensionality is embed_dim split across the heads\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "    \n",
    "        # num_heads Q,K and V matrices, initialized as one \"super head\"\n",
    "            # note: in model 5 , these 3 matrices are combined into one\n",
    "        self.key = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.query = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "    \n",
    "        #final linear projection merges the heads outputs\n",
    "        self.W0 = nn.Linear(embed_dim, embed_dim,bias=False)\n",
    "    \n",
    "    def forward(self, x, track_sizes=False):\n",
    "        # extract the dimension size of the inputs(token embedds)\n",
    "        B, T, E = x.shape # [batch, tokens (or seq_len), embed_dim]\n",
    "        if track_sizes: print(f\"1) {'Input data shape:' :>28} {x.shape}\")\n",
    "\n",
    "        #push data through Q,K and V (acutally mulitple heads still in same matrix)\n",
    "        q = self.query(x) # [batch, seq_len, embed_dim]\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        if track_sizes: print(f\"2) {'q/k/v pre-split shape':>28} {q.shape}\")\n",
    "\n",
    "        # reshape to split up the heads (note: head splitting done after X.Wq\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "        # but pytorch SDPA func needs the shape to be [B, num_heads, T, head_dim]\n",
    "        #(1,2) bcoz batch is 1st dim, and dont want to transpose batch dim\n",
    "        # we preserve batch, head_dim\n",
    "        q = q.transpose(1,2) \n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        if track_sizes: print(f\"3) {'q/k/v post-split shape':>28} {q.shape}\")\n",
    "\n",
    "        # now we call SDPA\n",
    "        out = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n",
    "        if track_sizes: print(f\"4) {'Data post attention shape':>28} {out.shape}\")\n",
    "\n",
    "        # but our code still needs [B,T,num_heads, head_dim]\n",
    "        out = out.transpose(1,2)\n",
    "        if track_sizes: print(f\"5) {'Post attention data reshape':>28} {out.shape}\")\n",
    "\n",
    "        # merge heads back into embed_dim\n",
    "        out = out.reshape(B, T, E) # this is the [A1A2A3...Ah] line in notebook\n",
    "        if track_sizes: print(f\"6) {'Data merged to size':>28} {out.shape}\")\n",
    "\n",
    "        #finallt apply linear mixing matrix\n",
    "        out = self.W0(out)\n",
    "        if track_sizes: print(f\"7) {'Post-MHA H0 linear mixing':>28} {out.shape}\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e85e5d4-b559-4998-a654-7f86477cb75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(n_heads, embed_dim)\n",
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f26e48c6-ec3b-4fc6-810c-741db26a26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:torch.Size([5, 8, 128])\n",
      "Output size:torch.Size([5, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "# run some fake data though\n",
    "data = torch.randn(size=(batch_size, seq_len,embed_dim))\n",
    "out = mha(data)\n",
    "print(f'Input size:{data.shape}')\n",
    "print(f'Output size:{out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60d7292b-f268-49fb-91de-092f30c18bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Seq length: 8\n",
      "embedding dimen: 128\n",
      "    No of heads: 4\n",
      "     Head dimensionality: 32\n",
      "\n",
      " Dimensions of data as iit passes through attention sublayer of one transformer block\n",
      "1)            Input data shape: torch.Size([5, 8, 128])\n",
      "2)        q/k/v pre-split shape torch.Size([5, 8, 128])\n",
      "3)       q/k/v post-split shape torch.Size([5, 4, 8, 32])\n",
      "4)    Data post attention shape torch.Size([5, 4, 8, 32])\n",
      "5)  Post attention data reshape torch.Size([5, 8, 4, 32])\n",
      "6)          Data merged to size torch.Size([5, 8, 128])\n",
      "7)    Post-MHA H0 linear mixing torch.Size([5, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "print(f'     Seq length: {seq_len}')\n",
    "print(f'embedding dimen: {embed_dim}')\n",
    "print(f'    No of heads: {n_heads:}')\n",
    "print(f'     Head dimensionality: {embed_dim // n_heads}')\n",
    "\n",
    "print('\\n Dimensions of data as iit passes through attention sublayer of one transformer block')\n",
    "out = mha(data,track_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143f2db-a638-4e9c-be47-4746c202f1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
