{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fcb9b4-421d-42d4-87e2-da9f91fb3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf872776-5e4c-4af9-a4ff-eb1caeeb54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f80b26-9e73-4a52-b701-e388ef47f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#use GPT2 tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d91deb9-e6fe-468b-bfe2-6606cd658a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparas\n",
    "seq_len = 8 # aka context length\n",
    "n_vocab = tokenizer.vocab_size #n\n",
    "\n",
    "#model hyperparas\n",
    "embed_dim = 128\n",
    "nTransformerBlocks = 12\n",
    "\n",
    "batch_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6003e90a-92f3-4b67-9cc9-c9e6adb9ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one attention head\n",
    "class OneAttentionHead(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # create q,k,v matrices\n",
    "        self.key = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.query = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.W0 = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #run the token embedd vectors through attention\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        y = F.scaled_dot_product_attention(q,k,v,is_causal=True)\n",
    "        #is_causal make sures the time causal mask is included in calculations\n",
    "        y = self.W0(y) #linear transfor\n",
    "\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2f938-f21b-4276-9e95-bfecdc3e26ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e08e294-8fe7-4cb6-982d-aab874fa2272",
   "metadata": {},
   "source": [
    "#n Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6690bbe-2fd9-4df7-b6c9-6c8a0c26b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        #attention sublayer\n",
    "        self.layerNormAttn = nn.LayerNorm(embed_dim)\n",
    "        self.attn = OneAttentionHead(embed_dim)\n",
    "\n",
    "        #feedfwd (MLP) sublayer\n",
    "        self.layerNormMLP = nn.LayerNorm(embed_dim)\n",
    "        self.W1 = nn.Linear(embed_dim,4*embed_dim) # 4x expansion\n",
    "        self.gelu = nn.GELU()\n",
    "        self.W2 = nn.Linear(4*embed_dim, embed_dim) #4x contraction\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        ## ----attention sublayer ------##\n",
    "        x = x + self.attn(self.layerNormAttn(x))\n",
    "        # ------------------------------#\n",
    "\n",
    "        # --------MLP sublayer -------#\n",
    "        y = x + self.W2(self.gelu(self.W1(self.layerNormMLP(x))))\n",
    "        #-----------------------------â€“#\n",
    "        # y is [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # y either goes to next transformer block\n",
    "        # or to final unembedding matrix and then to token and text\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a56c2-6e5a-47a1-acdc-1ca3e3793c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a1ba08d-c593-4f4c-99ba-55eea05b2bcc",
   "metadata": {},
   "source": [
    "The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec193bb-4730-4fd4-8f88-21a3d9b030c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full model class, which calls the previously defined classes\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, nTransformerBlocks, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding matrices\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.positions = nn.Embedding(seq_len, embed_dim)\n",
    "\n",
    "        #n mutliple Transformer blocks\n",
    "        # * is a unpacking operator, the list of txf blocks goes into input of Sequential()\n",
    "        self.transformerBlocks = nn.Sequential(*[TransformerBlock(embed_dim) for _ in range(nTransformerBlocks)])\n",
    "        # self.transformerBlocks is a Pytorch Sequential object that contain 12 txf blcoks\n",
    "\n",
    "        # embedding to output (linear) layer\n",
    "        self.finalLayerNorm = nn.LayerNorm(embed_dim) # final layernorm after all txf blocks\n",
    "        self.finalLinear = nn.Linear(embed_dim, n_vocab, bias=False)\n",
    "\n",
    "        #final ouput layer (unembedd) tied to token embedd\n",
    "        self.finalLinear.weight = nn.Parameter(self.embedding.weight)\n",
    "\n",
    "    def forward(self, tokx):\n",
    "\n",
    "        #----------embeddings-------------##\n",
    "        token_embed = self.embedding(tokx) \n",
    "        posit_embed = self.positions(torch.arange(tokx.shape[-1])) #[seq_len, embed_dim]\n",
    "        x = token_embed + posit_embed #[batch, seq_len,embed_dim]\n",
    "        ##--------------------------------##\n",
    "\n",
    "        #n\n",
    "        ##--transformer blocks----##\n",
    "        x = self.transformerBlocks(x)\n",
    "        ##-------------------------##\n",
    "\n",
    "        #-----finally unembeddings----##\n",
    "        x = self.finalLayerNorm(x)\n",
    "        x = self.finalLinear(x)\n",
    "        # x is [batch, seq_len, n_vocab]\n",
    "        return x\n",
    "\n",
    "    def generate(self,tokx,temperature=1.,n_new_tokens=50):\n",
    "        for _ in range(n_new_tokens):\n",
    "            x = self(tokx[:,-seq_len:]) # get preds, but only from past seq_len tokens\n",
    "            x = x[:,-1,:] #extract final token to predict the next\n",
    "\n",
    "            # apply softmaxt to get prob values over all tokens in vocab - with temp\n",
    "            probs = F.softmax(x/temperature,dim=-1)\n",
    "\n",
    "            #probabilistically sample from distbn\n",
    "            tokx_next = torch.multinomial(probs, num_samples=1) # [batch,1]\n",
    "            \n",
    "            #append \n",
    "            tokx = torch.cat((tokx, tokx_next),dim=1) #[batch, (tokens+1)]\n",
    "        return tokx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc643a2a-82f4-4c20-8b61-b18a42faffff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3424c4c1-23f4-4f04-a865-40acd89a4703",
   "metadata": {},
   "source": [
    "create a model instance and inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b832ac-2ee3-4aee-baef-4ebbddbdfbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(50257, 128)\n",
       "  (positions): Embedding(8, 128)\n",
       "  (transformerBlocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): OneAttentionHead(\n",
       "        (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (finalLayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (finalLinear): Linear(in_features=128, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = LanguageModel(nTransformerBlocks,embed_dim)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300dc279-35ca-4c4d-be87-1ad2d1c1d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (layerNormAttn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): OneAttentionHead(\n",
       "    (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       "  )\n",
       "  (layerNormMLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (W1): Linear(in_features=128, out_features=512, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (W2): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.transformerBlocks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355fe471-186c-4e30-91d4-579a57d9b6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneAttentionHead(\n",
       "  (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (query): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (value): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (W0): Linear(in_features=128, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.transformerBlocks[2].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cad8176-f086-4af7-8f28-ec5acb74214b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0487, -0.0691,  0.0671,  ..., -0.0229, -0.0550, -0.0579],\n",
       "        [ 0.0035,  0.0604, -0.0296,  ..., -0.0209, -0.0724, -0.0425],\n",
       "        [-0.0564, -0.0723, -0.0847,  ..., -0.0562,  0.0730,  0.0602],\n",
       "        ...,\n",
       "        [ 0.0515,  0.0644,  0.0506,  ..., -0.0121, -0.0835,  0.0549],\n",
       "        [-0.0323,  0.0249, -0.0067,  ...,  0.0657,  0.0785,  0.0063],\n",
       "        [-0.0679, -0.0389, -0.0240,  ..., -0.0848, -0.0562, -0.0834]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.transformerBlocks[2].attn.query.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832e87c-bf82-43c5-97d3-8bbafa29c17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6615bd6e-949d-4b54-9b24-239300f6301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# create data\n",
    "tokens = tokenizer.encode('I prefer oat milk in my coffee.')\n",
    "X = torch.tensor(tokens[:-1]).unsqueeze(0) #unsqueeze helps to have first dim as batch\n",
    "y = torch.tensor(tokens[1:]).unsqueeze(0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "940a0efb-79bd-43c0-a25e-9dfc30304912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 50257])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llm(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefb4b9-6852-4679-8920-05ea6599cd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
