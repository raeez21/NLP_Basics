{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b2339b-62f0-49d1-b91f-02d28a04b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b932d3f-693d-42dd-97ad-f2822f468b56",
   "metadata": {},
   "source": [
    "## Inspect tokenizer props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71782c4-baa9-4ba7-98d2-171639830b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_added_tokens_decoder',\n",
       " '_added_tokens_encoder',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_call_one',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_update_total_vocab_size',\n",
       " '_update_trie',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'basic_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'deprecation_warnings',\n",
       " 'do_basic_tokenize',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_message_with_chat_template',\n",
       " 'encode_plus',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'ids_to_tokens',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_chat_templates',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'total_vocab_size',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size',\n",
       " 'wordpiece_tokenizer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f490b49-2f80-4dd8-8b81-3a398067f3eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chunk',\n",
       " 'rigorous',\n",
       " 'blaine',\n",
       " '198',\n",
       " 'peabody',\n",
       " 'slayer',\n",
       " 'dismay',\n",
       " 'brewers',\n",
       " 'nz',\n",
       " '##jer',\n",
       " 'det',\n",
       " '##glia',\n",
       " 'glover',\n",
       " 'postwar',\n",
       " 'int',\n",
       " 'penetration',\n",
       " 'sylvester',\n",
       " 'imitation',\n",
       " 'vertically',\n",
       " 'airlift',\n",
       " 'heiress',\n",
       " 'knoxville',\n",
       " 'viva',\n",
       " '##uin',\n",
       " '390',\n",
       " 'macon',\n",
       " '##rim',\n",
       " '##fighter',\n",
       " '##gonal',\n",
       " 'janice',\n",
       " '##orescence',\n",
       " '##wari',\n",
       " 'marius',\n",
       " 'belongings',\n",
       " 'leicestershire',\n",
       " '196',\n",
       " 'blanco',\n",
       " 'inverted',\n",
       " 'preseason',\n",
       " 'sanity',\n",
       " 'sobbing',\n",
       " '##due',\n",
       " '##elt',\n",
       " '##dled',\n",
       " 'collingwood',\n",
       " 'regeneration',\n",
       " 'flickering',\n",
       " 'shortest',\n",
       " '##mount',\n",
       " '##osi',\n",
       " 'feminism',\n",
       " '##lat',\n",
       " 'sherlock',\n",
       " 'cabinets',\n",
       " 'fumbled',\n",
       " 'northbound',\n",
       " 'precedent',\n",
       " 'snaps',\n",
       " '##mme',\n",
       " 'researching',\n",
       " '##akes',\n",
       " 'guillaume',\n",
       " 'insights',\n",
       " 'manipulated',\n",
       " 'vapor',\n",
       " 'neighbour',\n",
       " 'sap',\n",
       " 'gangster',\n",
       " 'frey',\n",
       " 'f1',\n",
       " 'stalking',\n",
       " 'scarcely',\n",
       " 'callie',\n",
       " 'barnett',\n",
       " 'tendencies',\n",
       " 'audi',\n",
       " 'doomed',\n",
       " 'assessing',\n",
       " 'slung',\n",
       " 'panchayat',\n",
       " 'ambiguous',\n",
       " 'bartlett',\n",
       " '##etto',\n",
       " 'distributing',\n",
       " 'violating',\n",
       " 'wolverhampton',\n",
       " '##hetic',\n",
       " 'swami',\n",
       " 'histoire',\n",
       " '##urus',\n",
       " 'liable',\n",
       " 'pounder',\n",
       " 'groin',\n",
       " 'hussain',\n",
       " 'larsen',\n",
       " 'popping',\n",
       " 'surprises',\n",
       " '##atter',\n",
       " 'vie',\n",
       " 'curt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out some tokens\n",
    "all_tokens = list(tokenizer.get_vocab().keys())\n",
    "all_tokens[20000:20100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed7ee9e-4554-4c24-a3a5-64a9b8cd1a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2671"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "tokenizer.get_vocab()['science']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd50a5-0ea5-41b2-b9a9-91fff2d990f1",
   "metadata": {},
   "source": [
    "# Tokenizing a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22e203fc-3d36-4456-931a-9168060858ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2671\n",
      "2671\n"
     ]
    }
   ],
   "source": [
    "word = 'science'\n",
    "#tokenizng using 2 functions....they return same tokenIDs\n",
    "res1 = tokenizer.convert_tokens_to_ids(word)\n",
    "res2 = tokenizer.get_vocab()[word]\n",
    "\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff82efc-dd5a-4107-8db5-254bf18cd647",
   "metadata": {},
   "source": [
    "# Encoding a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49f867c-6882-4b69-b13b-699d8d2ac35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'science is great'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m res1 \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(text)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(res1)\n\u001b[0;32m----> 5\u001b[0m res2 \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(res1)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(res2)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'science is great'"
     ]
    }
   ],
   "source": [
    "text = 'science is great'\n",
    "\n",
    "res1 = tokenizer.convert_tokens_to_ids(text)\n",
    "print(res1)\n",
    "res2 = tokenizer.get_vocab()[text]\n",
    "\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b483b031-9d18-4acb-87c9-22db2348e0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 101 is \"[CLS]\"\n",
      "Token 2671 is \"science\"\n",
      "Token 2003 is \"is\"\n",
      "Token 2307 is \"great\"\n",
      "Token 102 is \"[SEP]\"\n",
      "\n",
      "science is great\n",
      "[CLS] science is great [SEP]\n"
     ]
    }
   ],
   "source": [
    "# get_vocab works for words and subwords not text\n",
    "#better way\n",
    "res3 = tokenizer.encode(text)\n",
    "\n",
    "for i in res3:\n",
    "    print(f'Token {i} is \"{tokenizer.decode(i)}\"')\n",
    "\n",
    "#[CLS] = classification\n",
    "#[SEP] = sentence separation\n",
    "\n",
    "print('')\n",
    "print(tokenizer.decode(res3, skip_special_tokens=True))\n",
    "print(tokenizer.decode(res3, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6faba721-37f0-4c56-9f36-68f4bc248037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [CLS] science is great [SEP] [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT adds [CLS]...[SEP] with each encode\n",
    "tokenizer.decode(tokenizer.encode(tokenizer.decode(tokenizer.encode(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d9cee-8be9-4138-834b-7494df368c01",
   "metadata": {},
   "source": [
    "# Calling the class directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "915ddb94-2a44-41f8-8e77-8e6a3961a83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2671, 2003, 2307, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can encode a text without having to call encode() member func\n",
    "# we can do this by calling the tokenizer class itself\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f7929b-9695-4f60-bb80-16dc2728182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2671, 2003, 2307, 102]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bc56df0-a367-464b-9e29-79fad17b19d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original snetnce\n",
      " AI is both exciting and terrifying\n",
      "\n",
      "Tokenised (segmented) sentence:\n",
      " ['ai', 'is', 'both', 'exciting', 'and', 'terrifying']\n",
      " [9932, 2003, 2119, 10990, 1998, 17082]\n",
      "\n",
      "Encoded from original text:\n",
      " [101, 9932, 2003, 2119, 10990, 1998, 17082, 102]\n",
      "\n",
      "\n",
      "decoded from token-wise encoding:\n",
      " ai is both exciting and terrifying\n",
      "\n",
      "decoded from text encoding:\n",
      " [CLS] ai is both exciting and terrifying [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## More on tokenizing \n",
    "\n",
    "sentence = 'AI is both exciting and terrifying'\n",
    "\n",
    "print('Original snetnce')\n",
    "print(f' {sentence}\\n')\n",
    "\n",
    "#segment the text into tokens\n",
    "tokenized = tokenizer.tokenize(sentence) # this return list of tokens, not tokenIDs\n",
    "print('Tokenised (segmented) sentence:')\n",
    "print(f' {tokenized}')\n",
    "\n",
    "\n",
    "#encode the tokenised sentece\n",
    "ids_from_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "print(f' {ids_from_tokens}\\n') # note no special tokens in here\n",
    "\n",
    "# and finally, encode from original sentence\n",
    "encodedText = tokenizer.encode(sentence)\n",
    "print('Encoded from original text:')\n",
    "print(f' {encodedText}\\n\\n')\n",
    "\n",
    "# now for decoding\n",
    "print('decoded from token-wise encoding:')\n",
    "print(f' {tokenizer.decode(ids_from_tokens)}\\n')\n",
    "\n",
    "print('decoded from text encoding:')\n",
    "print(f' {tokenizer.decode(encodedText)}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d6734ed-761b-4e41-a482-6479480fd7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'is', 'both', 'exciting', 'and', 'terrifying']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =sentence.split(' ')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b88276b4-d101-4d47-8069-16faab2765eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "i2 = tokenizer.convert_tokens_to_ids(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97251f1e-b4f1-48f7-9db6-ccb81a8df167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9932, 2003, 2119, 10990, 1998, 17082]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cb22a3b-59c2-4a99-af2b-873b1585b983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 2003, 2119, 10990, 1998, 17082]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82ed14c9-66b7-4166-af28-6f075f7ebda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'is', 'both', 'exciting', 'and', 'terrifying']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "850ff2fc-aa1b-492a-951e-a2b0440ab7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai', 'is', 'both', 'exciting', 'and', 'terrifying']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f74ae4-fcd9-4fdf-80bd-48f3c6d0101b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
