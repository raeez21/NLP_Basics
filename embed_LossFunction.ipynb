{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1278eb-a0f2-4d6d-9f15-9967e7fd232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09239d52-b0af-45c9-8e0f-e99fdd282532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ignore_index',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'reduction',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a loss fucntion isntance\n",
    "loss_function = nn.NLLLoss()\n",
    "dir(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3fca2dd-8761-4e34-be1d-c24bf5ffd9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model outputs:\n",
      " [-1.0, 2.3, 0.1]\n",
      "\n",
      "Log softmax outputs:\n",
      "  [-3.44, -0.14, -2.34] \n",
      "\n",
      "When correct output is index \"0\", the loss is 3.44\n",
      "When correct output is index \"1\", the loss is 0.14\n",
      "When correct output is index \"2\", the loss is 2.34\n"
     ]
    }
   ],
   "source": [
    "# start with 3 outputs (raw model outputs for 3 tokens in vocab)\n",
    "#imagine the model takes input data and gives 3 ougtputs corres. to 3 diff categories\n",
    "model_output = torch.tensor([[-1,2.3,.1]],dtype=torch.float64)\n",
    "print('Raw model outputs:')\n",
    "print(f' {model_output[0].tolist()}\\n')\n",
    "\n",
    "# NLLLoss expects log softmax model outputs as its inputs\n",
    "# applying softmax fun to above outputs to get softmax prob and take the log them (LOGSOFTMAX)\n",
    "logsoftmax_output = F.log_softmax(model_output, dim =-1)\n",
    "\n",
    "print('Log softmax outputs:')\n",
    "print(' ', [round(o.item(),2) for o in logsoftmax_output[0] ],'\\n')\n",
    "\n",
    "\n",
    "# check loss for diff targets\n",
    "for target in range(len(model_output[0])):\n",
    "    #which output is the target (correct response)?\n",
    "    target = torch.tensor([target])\n",
    "\n",
    "    # calculate loss\n",
    "    theloss = loss_function(logsoftmax_output, target)\n",
    "    print(f'When correct output is index \"{target.item()}\", the loss is {theloss.item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cb28e-5028-44ed-840a-89be874ed74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"When correct output is index \"0\", the loss is 3.44\"\n",
    "# This is the logsoftmax value for index [0]\n",
    "# bcoz the simlified eqn: is L = -ln(y^) and ln(y^) = -3.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347ae19-0c7c-4113-abcc-12ee9ce3d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why 2nd line didnot have loss=0 when model had correct predn???\n",
    "# that is the beauty of softmax\n",
    "     # its not a greedy \"take the highest value\" strategy, isntead:\n",
    "     # all other values (-3.44, -2.34) are also incorprated into sofmtax value for 2.3\n",
    "     # so model still learns and going to supress the other 2 value and make 2.3 even larger\n",
    "     # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49cb624-3f9e-42d7-a424-5839a7e32f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0][0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212bd3c-8c34-4d2a-853b-38aad7676913",
   "metadata": {},
   "source": [
    "## multi sample losses (for batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a0d7958-a00a-4d27-9e4d-0a3da0790629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000,  2.3000,  0.1000],\n",
      "        [-1.0000,  2.3000,  0.1000],\n",
      "        [-1.0000,  2.3000,  0.1000],\n",
      "        [-1.0000,  2.3000,  0.1000]], dtype=torch.float64) \n",
      "\n",
      "tensor([[-3.4377, -0.1377, -2.3377],\n",
      "        [-3.4377, -0.1377, -2.3377],\n",
      "        [-3.4377, -0.1377, -2.3377],\n",
      "        [-3.4377, -0.1377, -2.3377]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# lets crteate 4 batches\n",
    "batch_output = model_output.repeat(4,1)\n",
    "print(batch_output,'\\n')\n",
    "\n",
    "# but of course we need logsoftmax\n",
    "logsoftmax_output = F.log_softmax(batch_output,dim=1)\n",
    "print(logsoftmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea54f957-c1f5-472b-8617-de1e7cb73cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target changes for each batch\n",
    "targets = torch.tensor([0,1,2,0]) # target response for each row in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f4499d3-8090-4c09-84ce-b118afa4ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3377, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_function(logsoftmax_output, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bdad5-c1ec-45aa-9d6a-53634259ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when training in batches, all of the individual losses get avgd together [verified in below cell]\n",
    "# so batch learning is a form of regularisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbd2e54d-d3ea-4089-869c-fdfecba83b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3377])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.tensor([-3.4377 + -0.1377 + -2.3377 + -3.4377])/len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e7fb6bd-eaed-41d3-8e09-121faf6703b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "#loss func require a gradient\n",
    "# the model loss_function is not attached to a model or comp graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477e55e-d048-4e2d-816d-f914fb2ec1fe",
   "metadata": {},
   "source": [
    "## simple example in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7136bc77-d666-4c63-a07a-87d64e29cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1: loss = 1.463, weights = [-0.62, -0.18]\n",
      "Step  2: loss = 0.932, weights = [-0.31, -0.49]\n",
      "Step  3: loss = 0.610, weights = [-0.08, -0.72]\n",
      "Step  4: loss = 0.427, weights = [0.09, -0.89]\n",
      "Step  5: loss = 0.319, weights = [0.23, -1.03]\n",
      "Step  6: loss = 0.252, weights = [0.34, -1.14]\n",
      "Step  7: loss = 0.206, weights = [0.43, -1.23]\n",
      "Step  8: loss = 0.174, weights = [0.51, -1.31]\n",
      "Step  9: loss = 0.150, weights = [0.58, -1.38]\n",
      "Step 10: loss = 0.132, weights = [0.64, -1.44]\n"
     ]
    }
   ],
   "source": [
    "# define a weight matrix (requires_grad = True to track gradients)\n",
    "w=torch.tensor([[-1,.2]],requires_grad=True)\n",
    "\n",
    "# target category\n",
    "target = torch.tensor([0])\n",
    "\n",
    "#optmiser\n",
    "optimizer = torch.optim.SGD([w],lr=.5)\n",
    "\n",
    "#iterations\n",
    "numTrainingIters = 10\n",
    "\n",
    "#init some variables\n",
    "allWeights = torch.zeros((numTrainingIters+1,2))\n",
    "allWeights[0,:] = w.detach()\n",
    "allLosses = torch.zeros(numTrainingIters)\n",
    "\n",
    "#training loop\n",
    "for i in range(numTrainingIters):\n",
    "    #reset grads\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #model outputs (simulating a full model fwd pass)\n",
    "    modeloutput = F.log_softmax(w,dim=1)\n",
    "    \n",
    "    #loss\n",
    "    loss = loss_function(modeloutput,target)\n",
    "    allLosses[i] = loss.item()\n",
    "    \n",
    "    # Gradient Descent\n",
    "    loss.backward() # calculate gradient of loss w.r.t w\n",
    "    optimizer.step() # adjust w using SGD\n",
    "\n",
    "    #store new weights\n",
    "    allWeights[i+1,:] = w.detach()\n",
    "\n",
    "    #and print some results\n",
    "    print(f\"Step {i+1:2d}: loss = {loss.item():.3f}, weights = {[round(o.item(),2) for o in allWeights[i+1]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f834dc-766d-406c-b121-549f4fb829eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
