{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7605cfd-f99e-4738-907f-f775ffbb511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd3a130-bb94-4d1c-83ae-52267fc25aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use GPT2 tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018954cb-1f05-4888-94e3-cdf0172b772f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9b0dc8c-a2e6-4d52-b9b3-234b2859ab0a",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64644fdb-2527-49e3-a111-1bd8512d7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparas\n",
    "seq_len = 8 # aka context length\n",
    "n_vocab = tokenizer.vocab_size #n\n",
    "\n",
    "#model hyperparas\n",
    "embed_dim = 2**6 #64\n",
    "\n",
    "batch_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbdedfb1-faa2-4d3b-bbb0-180a2d512cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #embedding matrix\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.positions = nn.Embedding(seq_len, embed_dim) \n",
    "        \n",
    "        # final output linear layer (unembeddings)\n",
    "        self.finalLinear = nn.Linear(embed_dim, n_vocab, bias = False)\n",
    "        \n",
    "        # init the k,q,v matrices for attention\n",
    "        self.layernormA = nn.LayerNorm(embed_dim)\n",
    "        self.key = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.query = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.W0 = nn.Linear(embed_dim,embed_dim)\n",
    "\n",
    "        # final outout layer is tied to token embeddings\n",
    "        self.finalLinear.weight = nn.Parameter(self.embedding.weight)\n",
    "\n",
    "    def forward(self,tokx):\n",
    "        # create token+position embedding\n",
    "        token_embed = self.embedding(tokx) #[seq_len, embed_dim]\n",
    "        posit_embed = self.positions(torch.arange(tokx.shape[-1])) #[seq_len, embed_dim]\n",
    "\n",
    "        #their sum is the ouput of embeddings (the addition will broadcast for \n",
    "        x = token_embed + posit_embed #[batch, seq_len,embed_dim]\n",
    "\n",
    "        #n ------attention sublayer starts here\n",
    "        \n",
    "        #layernorm before attention\n",
    "        x = self.layernormA(x)\n",
    "\n",
    "        #attention algo\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        qk = q@k.transpose(-2,-1) # dot prod b/w query and keys\n",
    "        qk_scaled = qk* embed_dim**-.5\n",
    "\n",
    "        #apply mask for future tokens\n",
    "        pastmask = torch.tril(torch.ones(x.shape[0],seq_len,seq_len))\n",
    "        qk_scaled[pastmask==0] = -torch.inf\n",
    "\n",
    "        #softmaxify\n",
    "        qk_softmax = F.softmax(qk_scaled,dim=-1)\n",
    "\n",
    "        #final attention mechanism\n",
    "        y = qk_softmax @ v\n",
    "\n",
    "        y *= self.W0(y)\n",
    "\n",
    "        #n ------end attention\n",
    "\n",
    "        #--o--\n",
    "        # MLP sublayer would be here\n",
    "        # --o--\n",
    "\n",
    "        # y is now shape of [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # final output transformation (unembeddings)\n",
    "        y = self.finalLinear(y) / np.sqrt(embed_dim)\n",
    "        # now y is [batch, seq_len, n_vocab]\n",
    "        return y, (pastmask, qk_scaled, qk_softmax) #op some attention matrices for viz        \n",
    "        #hooks are better in real models to get part of the models like activations, matrices etc.\n",
    "        \n",
    "\n",
    "    def generate(self, tokx,temperature=1, n_new_tokens=50):\n",
    "        # tokx is [batch, tokens]\n",
    "\n",
    "        for _ in range(n_new_tokens):\n",
    "\n",
    "            # get predictions, but only from past seq_len tokens\n",
    "            x = self(tokx[:,-seq_len:])[0] # [batch, seq_len,n_vocab]\n",
    "            #model pushes into feed fwd task only the most recent 8 tokens\n",
    "            # begining it start with first 8 tokens you started with\n",
    "            # then it adds more and more tokens that model generates\n",
    "            # so, it generates new tokens based on tokens that itself has generated\n",
    "\n",
    "            \n",
    "            # extract the final token to predict the next\n",
    "            x = x[:,-1,:]  # [batch, vocab_size]\n",
    "            \n",
    "\n",
    "            # apply softmaxt to get prob values over all tokens in vocab - with temp\n",
    "            probs = F.softmax(x/temperature,dim=-1)\n",
    "\n",
    "            #probabilistically sample from distbn\n",
    "            tokx_next = torch.multinomial(probs, num_samples=1) # [batch,1]\n",
    "            # print(\"next token:\",tokenizer.decode([tokx_next]))\n",
    "            #append \n",
    "            tokx = torch.cat((tokx, tokx_next),dim=1) #[batch, (tokens+1)]\n",
    "        return tokx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47944e9c-b2ae-4f9d-986c-90e19bdab53a",
   "metadata": {},
   "source": [
    "Claculate logits (model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c288edb4-e127-499b-b2a3-9ed3e6ed7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# create data\n",
    "tokens = tokenizer.encode('I prefer oat milk in my coffee.')\n",
    "X = torch.tensor(tokens[:-1]).unsqueeze(0) #unsqueeze helps to have first dim as batch\n",
    "y = torch.tensor(tokens[1:]).unsqueeze(0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c1dd17e-d5cc-4422-9d04-3724be8a340f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 4702, 267, 265, 7545, 287, 616, 6891, 13]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6670f8d-8467-4545-94a5-69078bceda84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40, 4702,  267,  265, 7545,  287,  616, 6891]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d598e0e4-e576-44d6-b9b0-cf2a00b09876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "out, attn = model(X) # this calls forward(), not generate()\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8adb45-f4c1-4bbe-be43-92a62c2eeed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d2f027a-3c9f-4c71-ae2a-f003b514e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for random weights 10.825\n",
      "Observedd mean logsoftmax output: 10.829\n",
      "Cross entropy loss from pytorch  10.855\n"
     ]
    }
   ],
   "source": [
    "print(f'Expected loss for random weights {-np.log(1/tokenizer.vocab_size):.3f}')\n",
    "#this is pure by chance, so -ve log likelihood\n",
    "\n",
    "print(f'Observedd mean logsoftmax output: {torch.mean(-F.log_softmax(out.detach(),dim=-1)):.3f}')\n",
    "#take the output of model and take log softmax, avg of all observed results\n",
    "\n",
    "print(f'Cross entropy loss from pytorch  {F.cross_entropy(out.view(-1,out.shape[-1]), y.view(-1)):.3f}')\n",
    "\n",
    "\n",
    "#all3 are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17b670d4-620e-4024-ad0a-35fcf1abc84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time causal mask:\n",
      " tensor([[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "\n",
      "qk_scaled\n",
      " tensor([[[ 0.1337,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.3333, -0.4072,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [-0.2466,  0.3072,  0.0410,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.6310,  0.4456, -0.1337,  0.1302,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.1297, -0.0316, -0.3074, -0.0327,  0.4368,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.6467,  0.0744, -0.1543, -0.3090, -0.0118, -0.0961,    -inf,\n",
      "             -inf],\n",
      "         [-0.1792, -0.3290, -0.3364,  0.0769, -0.5122, -0.7752, -0.2893,\n",
      "             -inf],\n",
      "         [ 0.2218, -0.2241,  0.0123,  0.0115,  0.2164,  0.0180, -0.0357,\n",
      "           0.4582]]], grad_fn=<IndexPutBackward0>)\n",
      "\n",
      "qk_softmax\n",
      " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6771, 0.3229, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2455, 0.4272, 0.3273, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3445, 0.2863, 0.1604, 0.2088, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2125, 0.1808, 0.1372, 0.1806, 0.2889, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2949, 0.1664, 0.1324, 0.1134, 0.1526, 0.1403, 0.0000, 0.0000],\n",
      "         [0.1621, 0.1395, 0.1385, 0.2094, 0.1162, 0.0893, 0.1452, 0.0000],\n",
      "         [0.1406, 0.0900, 0.1140, 0.1139, 0.1399, 0.1147, 0.1087, 0.1781]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('time causal mask:\\n', attn[0])\n",
    "print('\\nqk_scaled\\n', attn[1])\n",
    "print('\\nqk_softmax\\n', attn[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239596be-be3e-497a-8f39-1e6f5af8bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b8ec1c-d806-401e-aa86-13d513fe7fc6",
   "metadata": {},
   "source": [
    "generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e638fa37-e5dd-4249-9f35-3fd7ecd12e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I grow up, I want to be a dismantle membr searches prayingalam awardingshirt corrupted gauiona'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'When I grow up, I want to be a'\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens = torch.tensor(tokens).unsqueeze(0)\n",
    "\n",
    "generated_tokens = model.generate(tokens,temperature=2,n_new_tokens=10)[0]\n",
    "\n",
    "tokenizer.decode(generated_tokens.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49bf5702-0cf6-4c17-864c-a12664c8161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=10\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        print(\"BEFORE,A is: \", a)\n",
    "        # a+=5\n",
    "        print(\"AFTER A is: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dce2210f-0450-41d7-9cd3-7c589a0188f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE,A is:  10\n",
      "AFTER A is:  10\n"
     ]
    }
   ],
   "source": [
    "ob= A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ea225-9998-4e9a-a6a4-64a695ba5b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ddfe9-2ba3-444a-87ec-5629ffbc5474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
