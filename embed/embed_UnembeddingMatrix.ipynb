{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfe3bcc-80f8-4d41-baed-2ef35e5db741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8373529e-a3c2-450d-a979-b00d38f666c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "#pretrained GPT2 model and tokenizer\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc60a5b-35ce-4dc5-ace7-8913fccb88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings matrix\n",
    "embeddings = gpt2.wte.weight.detach().numpy()  # the weight matrix is detached from gradient info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51107fa1-ad94-46a6-b0e8-9d7da60b645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find size para in .config\n",
    "gpt2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab02bc2e-6646-413e-9acb-30064e2f5e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embediing dimensions: 768\n",
      "Vocab size: 50257\n",
      "Size of embeddings matrix: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f'Embediing dimensions: {gpt2.config.n_embd}')\n",
    "print(f'Vocab size: {gpt2.config.vocab_size}')\n",
    "print(f'Size of embeddings matrix: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfbeb0-0099-4647-a97d-e1c533d97372",
   "metadata": {},
   "source": [
    "## creating 2 unembedding matrix\n",
    "## 1. transpose of embedding matrix\n",
    "## 2. random unembed matrix of same size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93060a0e-590a-446e-8542-e0ccd0073ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id of embeddings:  4398153328\n",
      "id of unembeddings:  14185713840\n"
     ]
    }
   ],
   "source": [
    "unembeddings = embeddings.T\n",
    "\n",
    "#confirm transposing creates a new copy, not same reference\n",
    "# if it was just unembed = embedd, this would just create a new reference to same object\n",
    "# not just actual copy. But since it is embed.T (an operation on embedd array), this creates\n",
    "# an actual copy with new object\n",
    "print('id of embeddings: ', id(embeddings))\n",
    "print('id of unembeddings: ', id(unembeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5783e2-807e-4cd1-9354-cd30feb172c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Size of embed matrix: (50257, 768)\n",
      " Size of random unembed matrix: (768, 50257)\n",
      " Size of real unembedd matrix: (768, 50257)\n"
     ]
    }
   ],
   "source": [
    "# random unembed matrix\n",
    "# sizze of 768 x 50257 (T of embed matrix)\n",
    "unembedRand = np.random.randn(gpt2.config.n_embd, gpt2.config.vocab_size)\n",
    "\n",
    "print(f'    Size of embed matrix: {embeddings.shape}')\n",
    "print(f' Size of random unembed matrix: {unembedRand.shape}')\n",
    "print(f' Size of real unembedd matrix: {unembeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc44e2-4382-4760-afa7-2ff32a86433d",
   "metadata": {},
   "source": [
    "## California embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adef27b1-5106-444d-a8aa-146d30457d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3442]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pciking a word\n",
    "seedword = ' California'\n",
    "\n",
    "#its token index\n",
    "seed_idx = tokenizer.encode(seedword)\n",
    "\n",
    "#make sure its one token\n",
    "seed_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54be732d-992b-4631-9bbf-d408c568399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding its embedding vector\n",
    "embed_vector = embeddings[seed_idx,:]\n",
    "embed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0601e271-3660-4838-800e-e72426e6da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_vector   X   unembeddings  =  dotproducts\n",
      "(1, 768)           (768, 50257)      (1, 50257)\n"
     ]
    }
   ],
   "source": [
    "# project the embedding vector into umbedd matrix\n",
    "# i.e dot product\n",
    "\n",
    "dpRand = embed_vector @ unembedRand\n",
    "\n",
    "#next token is the max dot product (unscaled cosine similarity, i.e cosine sim without denominator)\n",
    "nextTokenRand_idx = np.argmax(dpRand)\n",
    "nextTokenRand = tokenizer.decode(nextTokenRand_idx)\n",
    "\n",
    "# check the sizes\n",
    "print('embed_vector   X   unembeddings  =  dotproducts')\n",
    "print(f'{embed_vector.shape}           {unembedRand.shape}      {dpRand.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2303859b-d5d8-423a-8e00-1e5b2b9e7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for real unembedd matrix\n",
    "dpReal = embed_vector @ unembeddings\n",
    "nextTokenReal_idx = np.argmax(dpReal)\n",
    "nextTokenReal = tokenizer.decode(nextTokenReal_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dcad957-e5fd-4111-9546-a6baa4cacc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Random unembed matrix:\n",
      "    \" California\" has largest dot product with: \" Purpose\"\n",
      "** real unembed matrix:\n",
      "    \" California\" has largest dot product with: \"California\"\n"
     ]
    }
   ],
   "source": [
    "print('** Random unembed matrix:')\n",
    "print(f'    \"{tokenizer.decode(seed_idx)}\" has largest dot product with: \"{nextTokenRand}\"')\n",
    "\n",
    "print('** real unembed matrix:')\n",
    "print(f'    \"{tokenizer.decode(seed_idx)}\" has largest dot product with: \"{nextTokenReal}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "242acfa9-fd5c-4ab0-a266-db3fb24d9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## notice above that final predicted California did not have any space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d144396-d6d2-4e62-b221-649056345c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50257)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpReal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b62ea3f-e6a5-4733-a1bd-dc225f201401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product 10.136 for token \"California\"\n",
      "Dot product  9.617 for token \" California\"\n",
      "Dot product  8.816 for token \" Californ\"\n",
      "Dot product  8.088 for token \"Calif\"\n",
      "Dot product  7.718 for token \" Calif\"\n",
      "Dot product  7.359 for token \" Nevada\"\n",
      "Dot product  7.158 for token \"Arizona\"\n",
      "Dot product  7.151 for token \"Colorado\"\n",
      "Dot product  6.974 for token \"Florida\"\n",
      "Dot product  6.912 for token \"Oregon\"\n"
     ]
    }
   ],
   "source": [
    "# top 10 dot unembeds\n",
    "top10 = np.argsort(dpReal[0])[::-1][:10]\n",
    "for i in top10:\n",
    "    print(f'Dot product {dpReal[0,i]:6.3f} for token \"{tokenizer.decode(i)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bee63-1400-46a9-a3e7-abb997270a20",
   "metadata": {},
   "source": [
    "## Generating a token sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8940beda-0851-401e-853b-456dce814d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text: budget  budgetary  budgets  Budget  Treasurer Reviewer Downloadha 覚醒 Downloadha BuyableInstoreAndOnline\n"
     ]
    }
   ],
   "source": [
    "# sequence length\n",
    "seq_len = 10\n",
    "\n",
    "\n",
    "#initial seed\n",
    "nextword = 'budget'\n",
    "\n",
    "# init a list that will contain the text\\\n",
    "text = nextword\n",
    "\n",
    "# loop to create the seq\n",
    "for i in range(seq_len - 1):\n",
    "    # step 1 tokenize\n",
    "    token = tokenizer.encode(nextword)\n",
    "\n",
    "    #step2 get embedding vector\n",
    "    embed_vector = embeddings[token,:]\n",
    "\n",
    "    # step 3 project onto unmebed matrixx(dot products)\n",
    "    dp = embed_vector @ unembeddings\n",
    "\n",
    "    #step 4: find top 10 projectons\n",
    "    top10 = np.argsort(dp[0])[::-1][:10]\n",
    "\n",
    "    #step 5 randomly pick one for next token\n",
    "    aRandomToken = np.random.choice(top10)\n",
    "    nextword = tokenizer.decode(aRandomToken)\n",
    "\n",
    "    # step6: append the text\n",
    "    text += ' ' + nextword\n",
    "\n",
    "#print the final result\n",
    "print('Our text:',text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b9b19f5-3665-418b-8ab3-c228baee44c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text: budget  Prosecutor ohydrate  edited  transm gars cod 9  ruined bah\n"
     ]
    }
   ],
   "source": [
    "# repeat with random unembedd\n",
    "# sequence length\n",
    "seq_len = 10\n",
    "\n",
    "\n",
    "#initial seed\n",
    "nextword = 'budget'\n",
    "\n",
    "# init a list that will contain the text\\\n",
    "text = nextword\n",
    "\n",
    "# loop to create the seq\n",
    "for i in range(seq_len - 1):\n",
    "    # step 1 tokenize\n",
    "    token = tokenizer.encode(nextword)\n",
    "\n",
    "    #step2 get embedding vector\n",
    "    embed_vector = embeddings[token,:]\n",
    "\n",
    "    # step 3 project onto unmebed matrixx(dot products)\n",
    "    dp = embed_vector @ unembedRand\n",
    "\n",
    "    #step 4: find top 10 projectons\n",
    "    top10 = np.argsort(dp[0])[::-1][:10]\n",
    "\n",
    "    #step 5 randomly pick one for next token\n",
    "    aRandomToken = np.random.choice(top10)\n",
    "    nextword = tokenizer.decode(aRandomToken)\n",
    "\n",
    "    # step6: append the text\n",
    "    text += ' ' + nextword\n",
    "\n",
    "#print the final result\n",
    "print('Our text:',text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2d66169-9d0c-4d6c-b57d-aafefea96802",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DID WE JUST BUILT A CHATBOT??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb788c66-b355-42cb-8a2a-4c618d3c99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
