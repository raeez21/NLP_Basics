{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113e318b-b6c4-49a8-8bba-68175ddaa74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code to incorprate dropout reglrsn into model5\n",
    "    # - how to incorporate dropouit into multiple parts of an LLM\n",
    "    # - how to toggle dropout in attention algo for evaluation\n",
    "    # - imp of training set variability for training performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faeb53c-e720-4924-9a85-60d980317e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few cells below to show Fineweb dataset and how to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b47fef-59f1-4ce8-bc3f-78e73c3f96c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datatrove.pipeline.readers import ParquetReader  # to download the Fineweb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f3b66f-215c-48a7-bc6b-a94ce5d29638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many doc to retrive, each doc has ~750 tokens\n",
    "numDocs= 10\n",
    "\n",
    "# import the docs\n",
    "data_reader = ParquetReader('hf://datasets/HuggingFaceFW/fineweb/data',limit=numDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ae3fab-7b4d-4678-9749-28908075bf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 15:22:41.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mReading input file CC-MAIN-2013-20/000_00000.parquet, 1/27468\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'id',\n",
       " 'media',\n",
       " 'metadata',\n",
       " 'text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one dataset doc\n",
    "text = next(iter(data_reader()))\n",
    "dir(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda5eefd-64aa-4139-bd17-287aff5ccc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dump': 'CC-MAIN-2013-20',\n",
       " 'url': 'http://%20jwashington@ap.org/Content/Press-Release/2012/How-AP-reported-in-all-formats-from-tornado-stricken-regions',\n",
       " 'date': '2013-05-18T05:48:54Z',\n",
       " 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz',\n",
       " 'language': 'en',\n",
       " 'language_score': 0.9721424579620361,\n",
       " 'token_count': 717}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dad3ace-caca-45cd-a32b-f789250df20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How AP reported in all formats from tornado-stricken regionsMarch 8, 2012\\nWhen the first serious bout of tornadoes of 2012 blew through middle America in the middle of the night, they touched down in places hours from any AP bureau. Our closest video journalist was Chicago-based Robert Ray, who dropped his plans to travel to Georgia for Super Tuesday, booked several flights to the cities closest to the strikes and headed for the airport. He’d decide once there which flight to take.\\nHe never got on board a plane. Instead, he ended up driving toward Harrisburg, Ill., where initial reports suggested a town was destroyed. That decision turned out to be a lucky break for the AP. Twice.\\nRay was among the first journalists to arrive and he confirmed those reports -- in all formats. He shot powerful video, put victims on the phone with AP Radio and played back sound to an editor who transcribed the interviews and put the material on text wires. He then walked around the devastation with the Central Regional Desk on the line, talking to victims with the phone held so close that editors could transcribe his interviews in real time.\\nRay also made a dramatic image of a young girl who found a man’s prosthetic leg in the rubble, propped it up next to her destroyed home and spray-painted an impromptu sign: “Found leg. Seriously.”\\nThe following day, he was back on the road and headed for Georgia and a Super Tuesday date with Newt Gingrich’s campaign. The drive would take him through a stretch of the South that forecasters expected would suffer another wave of tornadoes.\\nTo prevent running into THAT storm, Ray used his iPhone to monitor Doppler radar, zooming in on extreme cells and using Google maps to direct himself to safe routes. And then the journalist took over again.\\n“When weather like that occurs, a reporter must seize the opportunity to get the news out and allow people to see, hear and read the power of nature so that they can take proper shelter,” Ray says.\\nSo Ray now started to use his phone to follow the storms. He attached a small GoPro camera to his steering wheel in case a tornado dropped down in front of the car somewhere, and took video of heavy rain and hail with his iPhone. Soon, he spotted a tornado and the chase was on. He followed an unmarked emergency vehicle to Cleveland, Tenn., where he was first on the scene of the storm's aftermath.\\nAgain, the tornadoes had struck in locations that were hours from the nearest AP bureau. Damage and debris, as well as a wickedly violent storm that made travel dangerous, slowed our efforts to get to the news. That wasn’t a problem in Tennessee, where our customers were well served by an all-formats report that included this text story.\\n“CLEVELAND, Tenn. (AP) _ Fierce wind, hail and rain lashed Tennessee for the second time in three days, and at least 15 people were hospitalized Friday in the Chattanooga area.”\\nThe byline? Robert Ray.\\nFor being adept with technology, chasing after news as it literally dropped from the sky and setting a standard for all-formats reporting that put the AP ahead on the most competitive news story of the day, Ray wins this week’s $300 Best of the States prize.\\n© 2013 The Associated Press. All rights reserved. Terms and conditions apply. See AP.org for details.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ff0d1-4e1c-498c-b60e-f188a328d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7692b8-b0c2-42e7-8dd7-c35a9f2fd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b8f86b-1391-466c-9448-382823c8f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "n_vocab = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c67bd0-e481-4eca-ae32-baebe39a2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper para for GPT2-124M\n",
    "embed_dim = 768 #embedding dim\n",
    "seq_len = 256 #max seq len\n",
    "n_heads = 12 # attention heads\n",
    "n_blocks = 12 # tranformer blocks\n",
    "#each transformer block has 12 atention heads\n",
    "batch_size = 8\n",
    "dropout = .1 #n dropoout proportion(10% dropout)\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d01e4a-ac41-4134-bcd9-517961b8e575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd47856-c90e-4c55-a7d0-59dcd8526703",
   "metadata": {},
   "source": [
    "Create a fineweb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad34f05-0f66-45a2-81b3-e1430a9f4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "numDocs = 1000 # each doc has ~750 tokens\n",
    "data_reader = ParquetReader('hf://datasets/HuggingFaceFW/fineweb/data',limit=numDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7d786e-e5b6-4eee-b288-2e042ae59744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 15:52:48.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.readers.base\u001b[0m:\u001b[36mread_files_shard\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mReading input file CC-MAIN-2013-20/000_00000.parquet, 1/27468\u001b[0m\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1848 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# join all texts into one token vector\n",
    "texttokens = np.array([])\n",
    "for t in data_reader():\n",
    "    texttokens = np.append(texttokens, tokenizer.encode(t.text))\n",
    "\n",
    "# need a pytorch tensor for trianing\n",
    "# texttokens = to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3b4ee1-38f7-4669-937b-64b085daf7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(748751,)\n"
     ]
    }
   ],
   "source": [
    "print(type(texttokens))\n",
    "print(texttokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a22cfe-28d4-425b-bfef-a77a33a73910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ther are 748,751 tokens, of which 34,895 are unique\n"
     ]
    }
   ],
   "source": [
    "#need a pytorch tensor for trianing\n",
    "texttokens = torch.tensor(texttokens, dtype=torch.long)\n",
    "\n",
    "#set() doesnt work on torch tensors\n",
    "print(f'\\nTher are {len(texttokens):,} tokens, of which {len(set(texttokens.numpy())):,} are unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15753cfa-404e-4ff8-80c9-b02073608e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6be6364d-08e0-40cb-a986-7c08ecd8c7c9",
   "metadata": {},
   "source": [
    "function to get training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3075be7a-e013-4a83-b7b6-d87b8b15c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data (size torch.Size([8, 256])):\n",
      " tensor([[ 9911,   503,   379,  ..., 40709, 38622,   318],\n",
      "        [ 5793,   373,  4615,  ...,   257,  3644,  1080],\n",
      "        [ 8128,   340,   373,  ...,   284,   262, 19154],\n",
      "        ...,\n",
      "        [ 3015,   284,  3051,  ...,  5318,   287,  5575],\n",
      "        [ 3607,   262,  5852,  ...,  9690,   284,   262],\n",
      "        [  714,   307,  2233,  ...,  8607, 25335,  3961]])\n",
      "\n",
      "\n",
      "Targets (size torch.Size([8, 256])):\n",
      " tensor([[  503,   379,   399,  ..., 38622,   318, 10488],\n",
      "        [  373,  4615,    11,  ...,  3644,  1080,  1900],\n",
      "        [  340,   373, 10403,  ...,   262, 19154,   338],\n",
      "        ...,\n",
      "        [  284,  3051,   287,  ...,   287,  5575, 48023],\n",
      "        [  262,  5852,  7353,  ...,   284,   262,  7059],\n",
      "        [  307,  2233,   284,  ..., 25335,  3961,   287]])\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "train_ratio = .9\n",
    "\n",
    "#index to split data\n",
    "test_split_point = int(train_ratio * len(texttokens))\n",
    "\n",
    "train_data = texttokens[:test_split_point]\n",
    "test_data = texttokens[test_split_point:]\n",
    "# in this split 10% of the last part is training set, whi9ch is not a good split\n",
    "# ideally we would want a random split\n",
    "\n",
    "\n",
    "# a func that return a batch of data samoples\n",
    "def get_data_batch(training = True):\n",
    "    # pick the dataset to use\n",
    "    if training:\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = test_data\n",
    "\n",
    "    #pick random idices to start\n",
    "    ix = torch.randint(len(data) - seq_len, size = (batch_size,))\n",
    "\n",
    "    #get the data and targets (via broadcasting outer product)\n",
    "    X = data[ix[:,None] + torch.arange(seq_len)]  # now this is becomes a matrix\n",
    "    y = data[ix[:,None] + torch.arange(1,seq_len+1)]\n",
    "\n",
    "    return X,y\n",
    "\n",
    "#example \n",
    "X,y = get_data_batch()\n",
    "print(f'Input data (size {X.shape}):\\n',X)\n",
    "print(f'\\n\\nTargets (size {y.shape}):\\n',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa08cc-796f-44e5-8790-8f958282a993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eefe035-9ea3-4435-b19c-705eead56b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(len(train_data) - seq_len, size = (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07a682cd-bc87-4bd2-9f15-a11e72ee34cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 97305, 307344, 535317, 503209, 639054, 254907, 375950, 155301])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c79235-521a-45de-a1fd-aa58d4b0877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97305,  97306,  97307,  ...,  97558,  97559,  97560],\n",
       "        [307344, 307345, 307346,  ..., 307597, 307598, 307599],\n",
       "        [535317, 535318, 535319,  ..., 535570, 535571, 535572],\n",
       "        ...,\n",
       "        [254907, 254908, 254909,  ..., 255160, 255161, 255162],\n",
       "        [375950, 375951, 375952,  ..., 376203, 376204, 376205],\n",
       "        [155301, 155302, 155303,  ..., 155554, 155555, 155556]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix[:,None] + torch.arange(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355f99df-5d72-475e-a756-f786b83d438b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 97306,  97307,  97308,  ...,  97559,  97560,  97561],\n",
       "        [307345, 307346, 307347,  ..., 307598, 307599, 307600],\n",
       "        [535318, 535319, 535320,  ..., 535571, 535572, 535573],\n",
       "        ...,\n",
       "        [254908, 254909, 254910,  ..., 255161, 255162, 255163],\n",
       "        [375951, 375952, 375953,  ..., 376204, 376205, 376206],\n",
       "        [155302, 155303, 155304,  ..., 155555, 155556, 155557]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix[:,None] + torch.arange(1,seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7832efa9-715f-4047-8606-1d3fec174541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  262,  4302,  1204,  ...,   995,   286,   262],\n",
       "        [  287,  2769, 10150,  ...,   481,   307,  2683],\n",
       "        [   13,   198,  3237,  ...,   284,  1064,   262],\n",
       "        ...,\n",
       "        [  284, 14658,   262,  ...,    11, 19010,   447],\n",
       "        [ 3521,   447,   247,  ...,   286,  1162,  2817],\n",
       "        [  257,  1474,    12,  ..., 24470,  5660,  2364]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[ix[:,None] + torch.arange(seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d285657d-024a-44a9-930f-7a5361fb38a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4302,  1204, 35445,  ...,   286,   262, 30477],\n",
       "        [ 2769, 10150,   329,  ...,   307,  2683,   546],\n",
       "        [  198,  3237,   286,  ...,  1064,   262, 29262],\n",
       "        ...,\n",
       "        [14658,   262,  5531,  ..., 19010,   447,   247],\n",
       "        [  447,   247,    83,  ...,  1162,  2817, 12023],\n",
       "        [ 1474,    12, 25833,  ...,  5660,  2364, 30061]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[ix[:,None] + torch.arange(1,seq_len+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816a8c6-b9f8-4fae-8a7a-4008e7202ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41094deb-7c6d-4a78-89c4-548765f92c8e",
   "metadata": {},
   "source": [
    "The model with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b803a85-7199-4c7c-9912-d5e00ed6f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        #head dimensionality is embed_dim split across the heads\n",
    "        self.num_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "    \n",
    "        # the three Q,K,V weight matrices are init as one, and are split inside attention eqn\n",
    "        self.QKV = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n",
    "    \n",
    "        #final linear projection merges the heads outputs\n",
    "        self.W0 = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # extract the dimension size of the inputs(token embedds)\n",
    "        B, T, E = x.shape # [batch, tokens (or seq_len), embed_dim]\n",
    "        \n",
    "\n",
    "        #push data through Q,K and V in one concatenated matrix\n",
    "        qkv = self.QKV(x) #[batch, seq_len, 3*embed]\n",
    "        q,k,v = torch.split(qkv, E, dim=2) # each matrix is [B,T,E]\n",
    "\n",
    "        # reshape to [B,T,nHeads, head_dim]\n",
    "        # and then transpose to [B, nHeads, T, head_dim]\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2) #[B, num_heads, T, head_dim]\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # Pytorchs SDPA func handles multi head shapes\n",
    "\n",
    "        # IMP-----> dropout enable for training\n",
    "        # self.training is an attribute inherited from nn.Module()\n",
    "        # print(\"B,T,E =\", B, T, E)\n",
    "        # print(\"q,k,v shapes:\", q.shape, k.shape, v.shape)\n",
    "        dropp = dropout if self.training == True else 0 \n",
    "        out = F.scaled_dot_product_attention(q,k,v,is_causal=True, dropout_p = dropp)\n",
    "\n",
    "        # recombine heads : (B,nHeads,T,head_dim) -> [B,T,E]\n",
    "        out = out.transpose(1,2).reshape(B,T,E)\n",
    "    \n",
    "\n",
    "        #finallt apply linear mixing matrix\n",
    "        out = self.W0(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #attention subblock\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim,eps=1e-5)\n",
    "        self.attn = MultiHeadAttention()\n",
    "\n",
    "        #feedfwd (MLP) sublayer\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim,eps=1e-5)\n",
    "        self.mlp_1 = nn.Linear(embed_dim,4*embed_dim,bias=True) # 4x expansion\n",
    "        self.gelu = nn.GELU()\n",
    "        self.mlp_2 = nn.Linear(4*embed_dim, embed_dim, bias=True) #4x contraction\n",
    "\n",
    "        self.trn_dropout = nn.Dropout(dropout)  # IMP--> Dropout here\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        ## ----attention sublayer ------##\n",
    "        x_att = self.layernorm_1(x) # pre attn normalisn\n",
    "        # IMP ---> Dropout below\n",
    "        x_att = x + self.trn_dropout(self.attn(x_att)) # run through attention, then add pre attn activations\n",
    "        # attention -> dropout-> add residual\n",
    "        \n",
    "        #MLP\n",
    "        x_ff = self.layernorm_2(x_att) # pre MLP normlsn\n",
    "        x_ff = self.mlp_2( self.gelu( self.mlp_1(x_ff))) # expansion-contraction\n",
    "        x_ff = x_att + self.trn_dropout(x_ff) # dropout the MLP and add back to embedd\n",
    "        \n",
    "        \n",
    "        return x_ff\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8773eebd-0235-4da5-a4b6-5954982eb7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention().training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe1b971-2691-4a36-bc63-f1e5b12dad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # token + posn embedds\n",
    "        self.wte = nn.Embedding(n_vocab, embed_dim) # token embedds\n",
    "        self.wpe = nn.Embedding(seq_len, embed_dim) # posn embedds\n",
    "        # IMP----> Dropout here\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #n mutliple Transformer blocks\n",
    "        self.transformerBlocks = nn.Sequential(*[TransformerBlock() for _ in range(n_blocks)])\n",
    "\n",
    "        # embedding to output (linear) layer\n",
    "        self.layernorm_final = nn.LayerNorm(embed_dim,eps=1e-5) # final layernorm after all txf blocks\n",
    "        #unembed matirx\n",
    "        self.final_head = nn.Linear(embed_dim, n_vocab, bias=False)\n",
    "        #final ouput layer (unembedd) tied to token embedd\n",
    "        self.final_head.weight = nn.Parameter(self.wte.weight)\n",
    "\n",
    "\n",
    "        self.apply(self.weightInits) #apply the input func (weightInits) iteratively to all elements of this class\n",
    "    \n",
    "    def weightInits(self, module):\n",
    "        \n",
    "        # init nn.Linear to normal with std=.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0, std=.02)\n",
    "\n",
    "            # init the bias terms to zero\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        # Init nn.Embeddings to Xavier\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "                \n",
    "    def forward(self, idx):\n",
    "\n",
    "        #----------embeddings-------------##\n",
    "        token_emb = self.wte(idx)  # [B,T,E]   T is seq_len and E is embed_dim\n",
    "        posit_emb = self.wpe(torch.arange(idx.shape[-1],device=device)) #[seq_len, embed_dim]\n",
    "        x = token_emb + posit_emb #[B,T,E]\n",
    "        x = self.emb_dropout(x) # IMP----> DROPOUT after summing E+P\n",
    "        ##--------------------------------##\n",
    "\n",
    "        #n\n",
    "        ##--pass through each transformer blocks----##\n",
    "        x = self.transformerBlocks(x)\n",
    "        ##-------------------------##\n",
    "\n",
    "        #-----finally unembeddings----##\n",
    "        x = self.layernorm_final(x)\n",
    "        logits = self.final_head(x) # [B,T, n_vocab]\n",
    "\n",
    "        #   IMP-----> we output logists here, no logsoftmax\n",
    "        outputs = logits/math.sqrt(embed_dim)\n",
    "        return outputs\n",
    "        # outputs is [batch, seq_len, n_vocab]\n",
    "\n",
    "    def generate(self,idx,temperature=1.,max_new_tokens=50):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # fwd passb\n",
    "            logits = self(idx[:,-seq_len:]) # [B,T,n_vocab]   get preds, but only from past seq_len tokens \n",
    "            logits = logits[:,-1,:] #[B,n_vocab]   extract last tokens logitsto predict the next\n",
    "\n",
    "            # apply softmax with temp to get prob values over all tokens in vocab - with temp\n",
    "            probs = F.softmax(logits/temperature,dim=-1) #[B,n_vocab]\n",
    "\n",
    "            #probabilistically sample next token from distbn\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # [batch,1]\n",
    "            \n",
    "            #append \n",
    "            idx = torch.cat((idx, idx_next),dim=1) #[batch, (tokens+1)]\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b80b60ad-30ea-448f-8e08-7fbe8bfd3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397bf88-fcdf-4dad-9728-c83a87f0bb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8097cd0-1616-429a-9767-f157ba4e0344",
   "metadata": {},
   "source": [
    "TRAIN on final token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc63adc-9e72-41df-be11-7c8b033920df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss and omptimizer funcitons\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "352285c6-f25c-4f46-865d-2596acd56f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: tensor(10.8214, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# check loss func with sizes\n",
    "X,y = get_data_batch()\n",
    "logits = model(X.to(device))\n",
    "\n",
    "# calculate losses on final target token (with logsoftmax\n",
    "# NOTE WE USED logsoftmax on logits in loss_func\n",
    "loss  = loss_function(F.log_softmax(logits[:,-1,:],dim=-1),y[:,-1].to(device))\n",
    "print('\\nLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc28ccc0-5997-47d9-8088-e4241adda50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  345,  2193,  1141,  ...,   901,    13,  1114],\n",
      "        [ 4271,  1900,   357,  ...,   250, 15645, 16929],\n",
      "        [   12, 31160,   198,  ...,    13,  1115,  9210],\n",
      "        ...,\n",
      "        [13838,    25, 21523,  ...,    17,  7133,   290],\n",
      "        [   25,  6182,    12,  ...,    12,  9413,  2214],\n",
      "        [ 9533,  6612,  3963,  ...,   683,  3830,   257]])\n",
      "tensor([ 1114, 16929,  9210,   340,   307,   290,  2214,   257])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f46b1-2e5d-4a38-b927-17ee4e3d3bf6",
   "metadata": {},
   "source": [
    "Now train the model!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a78454-10cb-4ef6-a6c0-bb6ced04c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 train loss: 10.824453353881836, test loss:m10.806079864501953\n",
      "Sample 80 train loss: 9.439631462097168, test loss:m8.874025344848633\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1234\n",
    "\n",
    "#init the loss\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for sampli in range(num_samples):\n",
    "    # get batch of data\n",
    "    X,y = get_data_batch()\n",
    "    \n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    #fwd pass\n",
    "    logits = model(X.to(device))\n",
    "    loss = loss_function(F.log_softmax(logits[:,-1,:],-1),y[:,-1].to(device))\n",
    "\n",
    "    #bckprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # sum the batch loss\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    if sampli%80==0:    \n",
    "        with torch.no_grad():\n",
    "            model.eval() # IMP-----> deactivate dropout during testing and eval\n",
    "            # NOTE: THe above will only deacticate dropout() class instances, not dropout using F.dropout function\n",
    "            # byut model.eval() sets self.training=False for MultiHeadAttention class\n",
    "            X,y = get_data_batch(False) # False -> testset data\n",
    "            out = model(X.to(device))\n",
    "            thisloss = loss_function(F.log_softmax(out[:,-1,:],-1),y[:,-1].to(device))\n",
    "            test_loss.append(thisloss.item())\n",
    "            model.train() # reactivate dropout\n",
    "            \n",
    "            print(f'Sample {sampli} train loss: {train_loss[-1]}, test loss:m{test_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307e327-ba34-46a8-8ab2-380c99b861ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f7545-97ba-46ce-a86d-62b9ec396492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482b73a-436f-4840-90f4-45d0ca986435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
