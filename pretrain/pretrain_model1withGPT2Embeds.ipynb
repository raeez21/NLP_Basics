{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871a9c64-e5d2-4312-b9a2-f21f17e0a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# in this notebook we copy embedd matrix from GPT2 into model 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71d538b-9d89-4feb-95ce-fb75e60a867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2's tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e60a83-0632-457f-91ca-594ad253383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2584574-dbe5-4332-ba40-68335160461f",
   "metadata": {},
   "source": [
    "Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124e4879-4af3-4f10-8e8f-b801eb3dfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparas\n",
    "seq_len = 8 # aka context length\n",
    "stride = 2\n",
    "\n",
    "#model hyperparas\n",
    "embed_dim = 768   # bcoz GPT2 embedd size is 768\n",
    "batch_size=64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c95be-6891-4068-92c5-8bd806020c70",
   "metadata": {},
   "source": [
    "Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6517e6-88e8-45dd-a7bf-dc73473d1ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (48533 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48533"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowload and tokenize text\n",
    "text = requests.get('https://www.gutenberg.org/files/35/35-0.txt').text\n",
    "tmTokens = torch.tensor(tokenizer.encode(text))\n",
    "len(tmTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146345fb-28f9-4b62-aa69-9ba891dc76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for a dataset (note: batching done by DataLoader, not here)\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_length=8, stride=4):\n",
    "\n",
    "        # init\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "\n",
    "        # overlapping seq of context_length\n",
    "        for i in range(0,len(tokens)-seq_length,stride):\n",
    "            # get context tokens and append to lists\n",
    "            self.inputs.append(tokens[i : i+seq_length])\n",
    "            self.targets.append(tokens[i+1 : i+seq_length+1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2606b52-72e5-43e2-ae61-7b4bb86b51f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 21836 sequences\n",
      "Test has 2427 sequences\n"
     ]
    }
   ],
   "source": [
    "# create an instnace\n",
    "token_dataset = TokenDataset(tmTokens, seq_len, stride)\n",
    "\n",
    "\n",
    "# spolit into train and test\n",
    "train_ratio = .9\n",
    "train_size = int(train_ratio * len(token_dataset))\n",
    "test_size = len(token_dataset) - train_size\n",
    "\n",
    "# create train/test subsets\n",
    "train_dataset, test_dataset = random_split(token_dataset, [train_size, test_size])\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "test_dataloader =  DataLoader(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "#Shuffle means everytime we call dataloader, it shuffle the seq of tokens ---> used to avoid systematic bias while training \n",
    "\n",
    "print(f'Train has {train_size} sequences')\n",
    "print(f'Test has {test_size} sequences')\n",
    "#Note: these are sequences, each seq has 8 tokens iside it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86574234-a57f-4e84-8fed-a0c3dbe5889d",
   "metadata": {},
   "source": [
    "The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f75a0a9-8bbc-4811-a864-f0ed75f3af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #embedding matrix\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, embed_dim)\n",
    "\n",
    "        #unembedding(linear layer)\n",
    "        self.gelu = nn.GELU() # non linearity\n",
    "        self.finalLinear = nn.Linear(embed_dim, tokenizer.vocab_size) # unembed layer\n",
    "\n",
    "    def forward(self, tokx):\n",
    "\n",
    "        #fwd pass\n",
    "        x = self.embedding(tokx) # [batch, token, embed_dim]\n",
    "        x = self.gelu(x)\n",
    "        x = self.finalLinear(x) # [batch, token, vocab_size]\n",
    "        \n",
    "        return F.log_softmax(x,dim=-1)\n",
    "        #NLLLoss expects input as log softmax values\n",
    "\n",
    "        \n",
    "    def generate(self, tokx, n_new_tokens=30):\n",
    "        # tokx is [batch, tokens]\n",
    "\n",
    "        for _ in range(n_new_tokens):\n",
    "\n",
    "            # get predictions\n",
    "            x = self(tokx) # reference to the model itself (i.e feed fwd sweep for tokx\n",
    "\n",
    "            # extract the final token to predict the next\n",
    "            x = x[:,-1,:]  # [batch, vocab_size]\n",
    "\n",
    "            #as log softmax has -ve values, it will create errors with multinomial()\n",
    "            # so we undo log but keep softmax\n",
    "            probs = torch.exp(x)\n",
    "\n",
    "            #probabilistically sample from distbn\n",
    "            nextToken = torch.multinomial(probs, num_samples=1) # [batch,1]\n",
    "            tokx = torch.cat((tokx, nextToken),dim=1) #[batch, (tokens+1)]\n",
    "        return tokx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41ee16-b878-4f7b-a5f4-6606f1d5bce2",
   "metadata": {},
   "source": [
    "Copy GPt2 embeddings matrix into model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483571a1-d33a-4e22-b318-19b9934fce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48802bea-1bff-4bc5-a212-6b946500a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58d1b4b-198d-4160-a63a-add0997458b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my model embedd: torch.Size([50257, 768])\n",
      "GPT2 model embedd: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# check if sizes mathc\n",
    "print(f'my model embedd: {model.embedding.weight.shape}')\n",
    "print(f'GPT2 model embedd: {gpt2.transformer.wte.weight.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a753019-49b5-48a6-ad73-4595a8406721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over\n",
    "model.embedding.weight.data = gpt2.transformer.wte.weight.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f9e81ea-4f09-4d0f-ac4a-df1a104cffb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad156606-c65d-4bcd-93fb-da687c620f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.wte.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d5b33b-c56e-4eae-9a31-a0a05d9afb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for equality by taking embedd for first token from both models\n",
    "model.embedding.weight[0] - gpt2.transformer.wte.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d179e60-5c6a-419d-956a-c0a03d5ad616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8043db98-ae51-4940-9d45-aeacbd66f9c0",
   "metadata": {},
   "source": [
    " train the data with frozen embedds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d465ae77-83f9-434a-94e6-be8e8a8c68e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# toggle this ON if you working on Exc3\n",
    "this_is_exercise3=True\n",
    "if this_is_exercise3:\n",
    "    # initial state\n",
    "    print(model.embedding.weight.requires_grad)\n",
    "\n",
    "    # switch it off\n",
    "    model.embedding.weight.requires_grad = False # this means this matrix wont be modified during training\n",
    "\n",
    "    # confirm\n",
    "    print(model.embedding.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "976b170c-31cc-4104-a4d0-6355f21c9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# create loss and omptimizer funcitons\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=.001, weight_decay=.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "665c2ea6-5aaf-41e1-9f04-cef096755380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch  1, train loss: 7.975837690788403, test loss: 5.9453694569437125\n",
      "Finished epoch  3, train loss: 4.669533461855169, test loss: 4.613303121767546\n",
      "Finished epoch  5, train loss: 4.127916941168713, test loss: 4.191250562667847\n",
      "Finished epoch  7, train loss: 3.7975460214224475, test loss: 3.9196625320534957\n",
      "Finished epoch  9, train loss: 3.5900646964011833, test loss: 3.7373640098069845\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "#init losses\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # init batch losses to accumulate\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # loop over batches in ddata loader\n",
    "    for X,y in train_dataloader:\n",
    "        #move data to GPU\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        # clear previous grads\n",
    "        model.zero_grad()\n",
    "\n",
    "        #fwd pass\n",
    "        log_probs = model(X)\n",
    "\n",
    "        #calculate losses on reshaped final target word\n",
    "        log_probs_flat = log_probs.view(-1, log_probs.shape[-1])  #tokens 0:N-1\n",
    "        y_flat = y.view(-1) # tokens 1:N\n",
    "        loss = loss_function(log_probs_flat,y_flat)\n",
    "\n",
    "        #bckprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # sum the batch loss\n",
    "        epoch_loss+= loss.item()\n",
    "        #loop over batches in epoch ends here\n",
    "\n",
    "    #evaluate the mode with test set\n",
    "    with torch.no_grad():   #IMP: this switches off all calculation related to grads in the model --> we use this while evaluation of model\n",
    "        testloss = 0\n",
    "        for X,y in test_dataloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            out = model(X)\n",
    "            out_flat = out.reshape(-1, out.shape[-1])\n",
    "            thisloss = loss_function(out_flat, y.view(-1))\n",
    "            testloss += thisloss.item()\n",
    "            \n",
    "\n",
    "    #scale byno of tokens in data loader\n",
    "    train_loss.append(epoch_loss/ len(train_dataloader))\n",
    "    test_loss.append(testloss/ len(test_dataloader))\n",
    "\n",
    "    if epoch%2==0:\n",
    "        print(f'Finished epoch {epoch+1:2}, train loss: {epoch_loss / len(train_dataloader)}, test loss: {testloss/len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42958809-ace9-4d3a-8419-65c28b7d76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, 'ks-', markerfacecolor='w',markersize=8,label='Train loss')\n",
    "plt.plot(test_loss, 'ro-', markerfacecolor='w',markersize=8,label='Test loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Loss')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
