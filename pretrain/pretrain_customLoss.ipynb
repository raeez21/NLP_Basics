{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07b5669-2e89-4d72-8872-f007a757ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30477c9c-fb56-4240-8364-c1907f1ac4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"The negative log likelihood loss. It is useful to train a classification\u001b[0m\n",
       "\u001b[0;34m    problem with `C` classes.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning\u001b[0m\n",
       "\u001b[0;34m    weight to each of the classes. This is particularly useful when you have an\u001b[0m\n",
       "\u001b[0;34m    unbalanced training set.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The `input` given through a forward call is expected to contain\u001b[0m\n",
       "\u001b[0;34m    log-probabilities of each class. `input` has to be a Tensor of size either\u001b[0m\n",
       "\u001b[0;34m    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`\u001b[0m\n",
       "\u001b[0;34m    with :math:`K \\geq 1` for the `K`-dimensional case. The latter is useful for\u001b[0m\n",
       "\u001b[0;34m    higher dimension inputs, such as computing NLL loss per-pixel for 2D images.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Obtaining log-probabilities in a neural network is easily achieved by\u001b[0m\n",
       "\u001b[0;34m    adding a  `LogSoftmax`  layer in the last layer of your network.\u001b[0m\n",
       "\u001b[0;34m    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra\u001b[0m\n",
       "\u001b[0;34m    layer.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`\u001b[0m\n",
       "\u001b[0;34m    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts\u001b[0m\n",
       "\u001b[0;34m    this class index (this index may not necessarily be in the class range).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. math::\u001b[0m\n",
       "\u001b[0;34m        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\\\\u001b[0m\n",
       "\u001b[0;34m        l_n = - w_{y_n} x_{n,y_n}, \\\\\u001b[0m\n",
       "\u001b[0;34m        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and\u001b[0m\n",
       "\u001b[0;34m    :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\u001b[0m\n",
       "\u001b[0;34m    (default ``'mean'``), then\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. math::\u001b[0m\n",
       "\u001b[0;34m        \\ell(x, y) = \\begin{cases}\u001b[0m\n",
       "\u001b[0;34m            \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\u001b[0m\n",
       "\u001b[0;34m            \\text{if reduction} = \\text{`mean';}\\\\\u001b[0m\n",
       "\u001b[0;34m            \\sum_{n=1}^N l_n,  &\u001b[0m\n",
       "\u001b[0;34m            \\text{if reduction} = \\text{`sum'.}\u001b[0m\n",
       "\u001b[0;34m        \\end{cases}\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        weight (Tensor, optional): a manual rescaling weight given to each\u001b[0m\n",
       "\u001b[0;34m            class. If given, it has to be a Tensor of size `C`. Otherwise, it is\u001b[0m\n",
       "\u001b[0;34m            treated as if having all ones.\u001b[0m\n",
       "\u001b[0;34m        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\u001b[0m\n",
       "\u001b[0;34m            the losses are averaged over each loss element in the batch. Note that for\u001b[0m\n",
       "\u001b[0;34m            some losses, there are multiple elements per sample. If the field :attr:`size_average`\u001b[0m\n",
       "\u001b[0;34m            is set to ``False``, the losses are instead summed for each minibatch. Ignored\u001b[0m\n",
       "\u001b[0;34m            when :attr:`reduce` is ``False``. Default: ``None``\u001b[0m\n",
       "\u001b[0;34m        ignore_index (int, optional): Specifies a target value that is ignored\u001b[0m\n",
       "\u001b[0;34m            and does not contribute to the input gradient. When\u001b[0m\n",
       "\u001b[0;34m            :attr:`size_average` is ``True``, the loss is averaged over\u001b[0m\n",
       "\u001b[0;34m            non-ignored targets.\u001b[0m\n",
       "\u001b[0;34m        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\u001b[0m\n",
       "\u001b[0;34m            losses are averaged or summed over observations for each minibatch depending\u001b[0m\n",
       "\u001b[0;34m            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\u001b[0m\n",
       "\u001b[0;34m            batch element instead and ignores :attr:`size_average`. Default: ``None``\u001b[0m\n",
       "\u001b[0;34m        reduction (str, optional): Specifies the reduction to apply to the output:\u001b[0m\n",
       "\u001b[0;34m            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\u001b[0m\n",
       "\u001b[0;34m            be applied, ``'mean'``: the weighted mean of the output is taken,\u001b[0m\n",
       "\u001b[0;34m            ``'sum'``: the output will be summed. Note: :attr:`size_average`\u001b[0m\n",
       "\u001b[0;34m            and :attr:`reduce` are in the process of being deprecated, and in\u001b[0m\n",
       "\u001b[0;34m            the meantime, specifying either of those two args will override\u001b[0m\n",
       "\u001b[0;34m            :attr:`reduction`. Default: ``'mean'``\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Shape::\u001b[0m\n",
       "\u001b[0;34m        - Input: :math:`(N, C)` or :math:`(C)`, where `C = number of classes`, `N = batch size`, or\u001b[0m\n",
       "\u001b[0;34m          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\u001b[0m\n",
       "\u001b[0;34m          in the case of `K`-dimensional loss.\u001b[0m\n",
       "\u001b[0;34m        - Target: :math:`(N)` or :math:`()`, where each value is\u001b[0m\n",
       "\u001b[0;34m          :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\u001b[0m\n",
       "\u001b[0;34m          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\u001b[0m\n",
       "\u001b[0;34m          K-dimensional loss.\u001b[0m\n",
       "\u001b[0;34m        - Output: If :attr:`reduction` is ``'none'``, shape :math:`(N)` or\u001b[0m\n",
       "\u001b[0;34m          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of K-dimensional loss.\u001b[0m\n",
       "\u001b[0;34m          Otherwise, scalar.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        >>> log_softmax = nn.LogSoftmax(dim=1)\u001b[0m\n",
       "\u001b[0;34m        >>> loss_fn = nn.NLLLoss()\u001b[0m\n",
       "\u001b[0;34m        >>> # input to NLLLoss is of size N x C = 3 x 5\u001b[0m\n",
       "\u001b[0;34m        >>> input = torch.randn(3, 5, requires_grad=True)\u001b[0m\n",
       "\u001b[0;34m        >>> # each element in target must have 0 <= value < C\u001b[0m\n",
       "\u001b[0;34m        >>> target = torch.tensor([1, 0, 4])\u001b[0m\n",
       "\u001b[0;34m        >>> loss = loss_fn(log_softmax(input), target)\u001b[0m\n",
       "\u001b[0;34m        >>> loss.backward()\u001b[0m\n",
       "\u001b[0;34m        >>>\u001b[0m\n",
       "\u001b[0;34m        >>>\u001b[0m\n",
       "\u001b[0;34m        >>> # 2D loss example (used, for example, with image inputs)\u001b[0m\n",
       "\u001b[0;34m        >>> N, C = 5, 4\u001b[0m\n",
       "\u001b[0;34m        >>> loss_fn = nn.NLLLoss()\u001b[0m\n",
       "\u001b[0;34m        >>> data = torch.randn(N, 16, 10, 10)\u001b[0m\n",
       "\u001b[0;34m        >>> conv = nn.Conv2d(16, C, (3, 3))\u001b[0m\n",
       "\u001b[0;34m        >>> log_softmax = nn.LogSoftmax(dim=1)\u001b[0m\n",
       "\u001b[0;34m        >>> # output of conv forward is of shape [N, C, 8, 8]\u001b[0m\n",
       "\u001b[0;34m        >>> output = log_softmax(conv(data))\u001b[0m\n",
       "\u001b[0;34m        >>> # each element in target must have 0 <= value < C\u001b[0m\n",
       "\u001b[0;34m        >>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\u001b[0m\n",
       "\u001b[0;34m        >>> # input to NLLLoss is of size N x C x height (8) x width (8)\u001b[0m\n",
       "\u001b[0;34m        >>> loss = loss_fn(output, target)\u001b[0m\n",
       "\u001b[0;34m        >>> loss.backward()\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m__constants__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reduction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/nn/modules/loss.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     NLLLoss2d"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.NLLLoss??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bbea7-81f9-4535-a32b-ee556c4574a9",
   "metadata": {},
   "source": [
    "Create and test custom loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76675129-dacc-4576-9f3b-3cf748c9fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLoss_L1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,yHat, y):\n",
    "\n",
    "        #L1 loss\n",
    "        l = torch.mean( torch.abs(yHat-y))\n",
    "\n",
    "        #correlation erro (just as an example possibilities\n",
    "        # l = 1 -torch.corrcoef(yHat,y)\n",
    "        return l\n",
    "\n",
    "class myLoss_L2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,yHat,y):\n",
    "\n",
    "        #L2 loss mean squared erro\n",
    "        l = torch.mean( (yHat-y)**2)\n",
    "        return l\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "652ba056-6afa-4278-bf16-6dbaaa7bf3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output= 2.0\n",
      "target value= 5\n",
      "L1 loss: 3.0\n",
      "L2 loss: 9.0\n"
     ]
    }
   ],
   "source": [
    "# create instances of loss classes\n",
    "lossfun1 = myLoss_L1()\n",
    "lossfun2 = myLoss_L2()\n",
    "\n",
    "# test values\n",
    "predicted_value = torch.tensor(2.)\n",
    "target_value = 5\n",
    "\n",
    "print(f'Model output= {predicted_value}')\n",
    "print(f'target value= {target_value}')\n",
    "print(f'L1 loss: {lossfun1(predicted_value, target_value)}')\n",
    "print(f'L2 loss: {lossfun2(predicted_value, target_value)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a2924-7eca-463a-8f27-21f99c7d77c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b1da32-02eb-4bed-95ba-d780b933b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.tensor([2.], requires_grad=True)\n",
    "w2 = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# target value\n",
    "target = torch.tensor([3.])\n",
    "\n",
    "# create optimizers\n",
    "learningrate  = .05\n",
    "optimizer1 = torch.optim.SGD([w1], lr=learningrate)\n",
    "optimizer2 = torch.optim.SGD([w2], lr=learningrate)\n",
    "\n",
    "numIters = 50\n",
    "\n",
    "all_losses = np.zeros((2,numIters))\n",
    "all_weights = np.zeros((2,numIters+1))\n",
    "all_weights[:,0]  = w1.item()\n",
    "\n",
    "for i in range(numIters):\n",
    "    optimizer1.zero_grad()\n",
    "    loss1= lossfun1(w1, target)\n",
    "    loss1.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "    all_losses[0,i] = loss1.item()\n",
    "    all_weights[0,i+1] = w1.item()\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "    loss2 = lossfun2(w2, target)\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "\n",
    "    all_losses[1,i] = loss2.item()\n",
    "    all_weights[1,i+1] = w2.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee4b1f2d-e337-4caa-a914-62760f87ead4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.        , 2.04999995, 2.0999999 , 2.14999986, 2.19999981,\n",
       "        2.24999976, 2.29999971, 2.34999967, 2.39999962, 2.44999957,\n",
       "        2.49999952, 2.54999948, 2.59999943, 2.64999938, 2.69999933,\n",
       "        2.74999928, 2.79999924, 2.84999919, 2.89999914, 2.94999909,\n",
       "        2.99999905, 3.049999  , 2.99999905, 3.049999  , 2.99999905,\n",
       "        3.049999  , 2.99999905, 3.049999  , 2.99999905, 3.049999  ,\n",
       "        2.99999905, 3.049999  , 2.99999905, 3.049999  , 2.99999905,\n",
       "        3.049999  , 2.99999905, 3.049999  , 2.99999905, 3.049999  ,\n",
       "        2.99999905, 3.049999  , 2.99999905, 3.049999  , 2.99999905,\n",
       "        3.049999  , 2.99999905, 3.049999  , 2.99999905, 3.049999  ,\n",
       "        2.99999905],\n",
       "       [2.        , 2.0999999 , 2.18999982, 2.27099991, 2.34389997,\n",
       "        2.4095099 , 2.46855903, 2.52170324, 2.56953287, 2.61257958,\n",
       "        2.65132165, 2.68618941, 2.71757054, 2.74581361, 2.77123237,\n",
       "        2.79410911, 2.81469822, 2.83322835, 2.84990549, 2.86491489,\n",
       "        2.87842345, 2.89058113, 2.90152311, 2.91137075, 2.92023373,\n",
       "        2.92821026, 2.93538928, 2.94185042, 2.94766545, 2.95289898,\n",
       "        2.95760918, 2.96184826, 2.96566343, 2.96909714, 2.97218752,\n",
       "        2.97496867, 2.97747183, 2.97972465, 2.98175216, 2.98357701,\n",
       "        2.98521924, 2.98669744, 2.98802781, 2.98922515, 2.99030256,\n",
       "        2.99127221, 2.99214506, 2.99293065, 2.99363756, 2.9942739 ,\n",
       "        2.99484658]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf18164-a9bc-42d1-9739-280e360470ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 9.50000048e-01, 9.00000095e-01, 8.50000143e-01,\n",
       "        8.00000191e-01, 7.50000238e-01, 7.00000286e-01, 6.50000334e-01,\n",
       "        6.00000381e-01, 5.50000429e-01, 5.00000477e-01, 4.50000525e-01,\n",
       "        4.00000572e-01, 3.50000620e-01, 3.00000668e-01, 2.50000715e-01,\n",
       "        2.00000763e-01, 1.50000811e-01, 1.00000858e-01, 5.00009060e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02, 9.53674316e-07, 4.99989986e-02,\n",
       "        9.53674316e-07, 4.99989986e-02],\n",
       "       [1.00000000e+00, 8.10000181e-01, 6.56100273e-01, 5.31441152e-01,\n",
       "        4.30467248e-01, 3.48678559e-01, 2.82429516e-01, 2.28767782e-01,\n",
       "        1.85301945e-01, 1.50094584e-01, 1.21576592e-01, 9.84770879e-02,\n",
       "        7.97664002e-02, 6.46107197e-02, 5.23346290e-02, 4.23910618e-02,\n",
       "        3.43367495e-02, 2.78127827e-02, 2.25283615e-02, 1.82479862e-02,\n",
       "        1.47808567e-02, 1.19724888e-02, 9.69769713e-03, 7.85514340e-03,\n",
       "        6.36265846e-03, 5.15376683e-03, 4.17454494e-03, 3.38137313e-03,\n",
       "        2.73890491e-03, 2.21850607e-03, 1.79698190e-03, 1.45555532e-03,\n",
       "        1.17899978e-03, 9.54986899e-04, 7.73534121e-04, 6.26567402e-04,\n",
       "        5.07518533e-04, 4.11090004e-04, 3.32983764e-04, 2.69714510e-04,\n",
       "        2.18470857e-04, 1.76958230e-04, 1.43333309e-04, 1.16097413e-04,\n",
       "        9.40402897e-05, 7.61742995e-05, 6.17000624e-05, 4.99757007e-05,\n",
       "        4.04806196e-05, 3.27882117e-05]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e40d3c-5efc-45ef-877d-0cac552307b3",
   "metadata": {},
   "source": [
    "L2 loss is smoother than L1 loss\n",
    "towards the end, the closer it gets to target, the smaller the squared term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec5598-40cf-4b0f-b420-ee0957e2a519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
