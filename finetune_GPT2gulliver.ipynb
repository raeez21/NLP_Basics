{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b3aa3d-65b0-49c0-8831-99110c4a0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take a pretrainied GPT2 model and fine tune on Gullivers travels text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a66169-abfc-40a3-8486-315fb529a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModelForCausalLM, GPT2Tokenizer\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85472205-4bcc-46be-9c0e-34fb03c396d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 148/148 [00:00<00:00, 1786.50it/s, Materializing param=\n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gpt2  = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb486b9-ea4c-468b-b9bd-a8d86bddc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 16\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c493-de48-4a17-9b74-a665c39b9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2809 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2809])\n",
      "torch.Size([1, 2809])\n",
      "torch.Size([2809])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the text\n",
    "text = requests.get('https://www.gutenberg.org/cache/epub/829/pg289.txt').text\n",
    "\n",
    "# the old way\n",
    "gtTokens = torch.tensor(tokenizer.encode(text), dtype = torch.long)  # the output of .encode is Py list(), we then covnert to tensor\n",
    "print(gtTokens.shape)\n",
    "\n",
    "# a better way\n",
    "gtTokens = tokenizer.encode(text, return_tensors='pt') # Now this outputs a Pytorch tensor ('pt')\n",
    "# but this has a shape (singleton dimension)\n",
    "print(gtTokens.shape)\n",
    "\n",
    "# but rest of code ois setup for dimensionless tensors\n",
    "gtTokens =gtTokens[0]\n",
    "print(gtTokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ae7f8a-4a06-4ec7-8ca8-5b76d4b99dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token   220 appears 730 times and is \" \"\n",
      "Token   198 appears 156 times and is \"\n",
      "\"\n",
      "Token  2625 appears 124 times and is \"=\"\"\n",
      "Token     1 appears 99 times and is \"\"\"\n",
      "Token    12 appears 78 times and is \"-\"\n",
      "Token  1279 appears 75 times and is \" <\"\n",
      "Token    13 appears 67 times and is \".\"\n",
      "Token    29 appears 63 times and is \">\"\n",
      "Token    64 appears 49 times and is \"a\"\n",
      "Token  5320 appears 39 times and is \"\">\"\n",
      "Token    14 appears 35 times and is \"/\"\n",
      "Token  1875 appears 32 times and is \" >\"\n",
      "Token  1398 appears 31 times and is \" class\"\n",
      "Token  3556 appears 27 times and is \"</\"\n",
      "Token 35922 appears 27 times and is \"=\"/\"\n",
      "Token  7359 appears 26 times and is \" </\"\n",
      "Token 13291 appears 26 times and is \" href\"\n",
      "Token  7146 appears 22 times and is \"div\"\n",
      "Token    62 appears 18 times and is \"_\"\n",
      "Token    20 appears 16 times and is \"5\"\n",
      "Token    70 appears 14 times and is \"g\"\n",
      "Token 28961 appears 13 times and is \"meta\"\n",
      "Token    11 appears 13 times and is \",\"\n",
      "Token  2695 appears 13 times and is \" content\"\n",
      "Token 12947 appears 12 times and is \"search\"\n",
      "Token  4528 appears 12 times and is \"li\"\n",
      "Token   657 appears 12 times and is \" 0\"\n",
      "Token    25 appears 11 times and is \":\"\n",
      "Token  8726 appears 11 times and is \"link\"\n",
      "Token 18242 appears 11 times and is \"label\"\n",
      "Token  1438 appears 10 times and is \" name\"\n",
      "Token    27 appears 10 times and is \"<\"\n",
      "Token  6494 appears 10 times and is \"html\"\n",
      "Token   628 appears 10 times and is \"\n",
      "\n",
      "\"\n",
      "Token 19028 appears 9 times and is \"utenberg\"\n",
      "Token    82 appears 9 times and is \"s\"\n",
      "Token 20336 appears 9 times and is \" Gutenberg\"\n",
      "Token  2099 appears 9 times and is \" type\"\n",
      "Token 15414 appears 8 times and is \"input\"\n",
      "Token 30487 appears 8 times and is \"/\"\"\n",
      "Token   642 appears 8 times and is \" 5\"\n",
      "Token   378 appears 8 times and is \"ate\"\n",
      "Token 10755 appears 8 times and is \"about\"\n",
      "Token  3119 appears 8 times and is \" property\"\n",
      "Token  2503 appears 7 times and is \"www\"\n",
      "Token    16 appears 7 times and is \"1\"\n",
      "Token    79 appears 7 times and is \"p\"\n",
      "Token 44256 appears 7 times and is \"toggle\"\n",
      "Token   257 appears 7 times and is \" a\"\n",
      "Token  2902 appears 7 times and is \"down\"\n",
      "Token   126 appears 6 times and is \"�\"\n",
      "Token   519 appears 6 times and is \"og\"\n",
      "Token 16497 appears 6 times and is \"ebook\"\n",
      "Token  9099 appears 6 times and is \"don\"\n",
      "Token  2398 appears 6 times and is \"org\"\n",
      "Token    66 appears 6 times and is \"c\"\n",
      "Token  1378 appears 6 times and is \"://\"\n",
      "Token 28112 appears 6 times and is \"!--\"\n",
      "Token  1478 appears 6 times and is \" 14\"\n",
      "Token    75 appears 6 times and is \"l\"\n",
      "Token  4686 appears 5 times and is \" id\"\n",
      "Token   952 appears 5 times and is \"io\"\n",
      "Token 14781 appears 5 times and is \"drop\"\n",
      "Token 16539 appears 5 times and is \"button\"\n",
      "Token  2682 appears 5 times and is \"34\"\n",
      "Token 16775 appears 5 times and is \"Project\"\n",
      "Token   687 appears 5 times and is \"form\"\n",
      "Token  5450 appears 5 times and is \"https\"\n",
      "Token  7496 appears 5 times and is \"ria\"\n",
      "Token  6404 appears 5 times and is \"log\"\n",
      "Token  4749 appears 5 times and is \"icon\"\n",
      "Token  6927 appears 5 times and is \"><\"\n",
      "Token 25677 appears 5 times and is \"header\"\n",
      "Token    21 appears 5 times and is \"6\"\n",
      "Token    78 appears 5 times and is \"o\"\n",
      "Token    71 appears 5 times and is \"h\"\n",
      "Token  2079 appears 4 times and is \"99\"\n",
      "Token   262 appears 4 times and is \" the\"\n",
      "Token   860 appears 4 times and is \" 9\"\n",
      "Token 26129 appears 4 times and is \"ategories\"\n",
      "Token    87 appears 4 times and is \"x\"\n",
      "Token  1983 appears 4 times and is \"27\"\n",
      "Token 30586 appears 4 times and is \"policy\"\n",
      "Token   284 appears 4 times and is \" to\"\n",
      "Token 34507 appears 4 times and is \"-.\"\n",
      "Token 11134 appears 4 times and is \"png\"\n",
      "Token 10506 appears 4 times and is \"ibl\"\n",
      "Token  4935 appears 4 times and is \" Project\"\n",
      "Token    15 appears 4 times and is \"0\"\n",
      "Token 12417 appears 4 times and is \"main\"\n",
      "Token  3670 appears 4 times and is \" title\"\n",
      "Token   345 appears 4 times and is \" you\"\n",
      "Token   329 appears 4 times and is \" for\"\n",
      "Token   486 appears 3 times and is \"01\"\n",
      "Token    80 appears 3 times and is \"q\"\n",
      "Token  5988 appears 3 times and is \" alt\"\n",
      "Token 13812 appears 3 times and is \"display\"\n",
      "Token  8585 appears 3 times and is \"About\"\n",
      "Token    22 appears 3 times and is \"7\"\n",
      "Token  9060 appears 3 times and is \"image\"\n"
     ]
    }
   ],
   "source": [
    "# most freq 100 tokens\n",
    "uniq, counts = np.unique(gtTokens, return_counts=True)\n",
    "freqidx = np.argsort(counts)[::-1]\n",
    "top100 = uniq[freqidx[:100]]\n",
    "\n",
    "for t in top100:\n",
    "    print(f'Token {t:5} appears {torch.sum(gtTokens==t)} times and is \"{tokenizer.decode(t)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a554d977-0b96-47bb-9fd6-fdb14036ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = gpt2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0508f565-d1f5-49b0-be5c-897ed8a5f87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37076f13-084c-4596-9de4-0f380eab6aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe that the State Department should feel threatened with these claims. They have made so many promises. I have not been able to find any records and I have nothing to prove to them of any such information. My concern is that they're going to force me to make public these conversations to the point where they can get a fair trial, I have no money for lawyers to help me and I don't want to see me killed. The last thing I want is if the President is going to\n"
     ]
    }
   ],
   "source": [
    "prompt = 'I cannot believe that'\n",
    "in2gpt = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "output = gpt2.generate(in2gpt, max_length=100, pad_token_id=50256,do_sample=True).cpu()\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1c7b0-0f69-4d97-be85-f72298bc8f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba2edae9-70b4-44ca-893e-bdae29a2b360",
   "metadata": {},
   "source": [
    "Quantify the freq of GT freq tokens in GPT's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "decd7be6-94e0-4777-9933-49fd3c28dfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18987,   508,   389,  ...,   383,  1812,  2732],\n",
      "        [43375,   262,  1438,  ...,   418,   198,   198],\n",
      "        [22983,   345,  1183,  ...,   422,   326,  1295],\n",
      "        ...,\n",
      "        [ 8515,   284,   383,  ..., 18708,  1201,   262],\n",
      "        [27956,    13,   405,  ...,    13,  9746,    15],\n",
      "        [17620,   287,  3012,  ...,  1720,  4433, 21771]]) \n",
      "\n",
      "\n",
      "*** Next batch ofg outoput\n",
      " Users who are willing to provide additional information on the issue: We are confident we'll find out right away.\n",
      "\n",
      "The Department of Homeland Security, Office of the Inspector General for Terrorist Threats, and the Inspector General for Immigration, Refugees, and Citizenship, have already done their best to investigate these types of incidents. They're well-versed in the law, understand how incidents can rise quickly and are dedicated to responding to the highest standards of safety, security, and security. The State Department\n",
      "\n",
      "*** Next batch ofg outoput\n",
      "unsigned the name of the server. For example,\n",
      "\n",
      "server: localhost:3000\n",
      "\n",
      "For the other servers it may take several iterations, in which case you may run into issues with the following:\n",
      "\n",
      "server: /usr/share/apache2/bin/mysql [127.0.0.1:8080] ... fail. You can either restart, restart your app, or use mysql. For example,\n",
      "\n",
      "server: /users/cameos\n",
      "\n",
      "\n",
      "\n",
      "*** Next batch ofg outoput\n",
      " Hussein you'll know. They didn't have to do so.\"\n",
      "\n",
      "And from the moment they met to the moment they fought, in an effort to save themselves, in hopes of defeating Gaddafi, and in hopes of building a new country, they all spoke to each other. They talked about the future, the future of their country and it was clear they would all make that transition. Gaddafi had always wanted to see a modern Libya, and they were convinced that he wanted to rebuild himself from that place\n",
      "\n",
      "*** Next batch ofg outoput\n",
      " Interactive (2000, THQ) (GBC) The Adventures of the Dimensional Beast II (2001, NOS) (Apple IIe) The Adventures of the X-Men of Marvel Age of the Atom 2 (WASOMAKONOMO) (1983, Atari) (TRS-80) The Adventures of the X-Men of Marvel Age of the X-Men: The Last Man (2003, SCEA (SCEA)) (PS2) The Adventures of\n",
      "\n",
      "*** Next batch ofg outoput\n",
      " slows of around one meter per second.\n",
      "\n",
      "The ability to get an additional attack attack in an instant is only supported by the new \"Sprint Speed\" ability. This allows opponents to easily and consistently slow down opponents' characters, which can help them recover from an onslaught of attacks, including the new \"Shiny\" attack. The ability was revealed at BlizzCon 2015 which saw the team introducing the newly introduced Sprint Speed at BlizzCon 2016\n",
      "\n",
      "It is stated in the Japanese version that the\n",
      "\n",
      "*** Next batch ofg outoput\n",
      " shrug\n",
      "\n",
      "You were able to control the fire. And when it went out, a new path formed.\n",
      "\n",
      "Yes, I didn't say it's a new path, but I can tell you it's a path in which the flames of your life were extinguished so that you can not take the death that was brought about by your own failures by others – the failures of my people, your problems, your sins.\n",
      "\n",
      "Yes, so far I've managed to escape.\n",
      "\n",
      "You don\n",
      "\n",
      "*** Next batch ofg outoput\n",
      "Thor-like energy is now released from the same atoms and molecules the sun-god has unleashed.\n",
      "\n",
      "NASA/JPL-Caltech/University of California, Irvine: With the arrival of the new technology, scientists can now simulate a physical model of \"sun energy\" by building a mini-thermite in a laboratory outside of earth's atmosphere.\n",
      "\n",
      "Now scientists could actually work with the sun to mimic what the scientists say is a natural phenomenon known as solar radiation, or sunspot intensity\n",
      "\n",
      "*** Next batch ofg outoput\n",
      " upgrade to The Elder Scrolls V: Skyrim for PC.\n",
      "\n",
      "After that date update, the game may never get a PC port.\n",
      "\n",
      "The Elder Scrolls V: Skyrim will be available next year for both Windows and Mac on PC.\n",
      "\n",
      "The game has been in development since March 2014, but after publishing for a bit, Bethesda decided not to ship the game around the world in October.\n",
      "\n",
      "While a patch will come out in September, the game was not considered a complete overhaul since the\n",
      "\n",
      "*** Next batch ofg outoput\n",
      "1016.0059.1953-05.ecex?sa=1546\n",
      "\n",
      "The main idea of that study was to look into whether testosterone had a long-term and statistically significant association between the risk of a hip fracture or femoral neck fracture and the risk of developing certain conditions from baseline to postmenopausal ( ). The analysis revealed that the risk of hip fracture (and other hip fractures) was associated with higher testosterone levels (β=− −0.9 vs. −0\n",
      "\n",
      "*** Next batch ofg outoput\n",
      "channel in Google\n",
      "\n",
      "• Get more email alerts\n",
      "\n",
      "• Google Calendar integration\n",
      "\n",
      "• Email for notifications\n",
      "\n",
      "• Google Now integration\n",
      "\n",
      "• Email support for your next Google event\n",
      "\n",
      "• One-click update with the latest updates on Facebook\n",
      "\n",
      "• Share your events to other friends using the latest email and SMS messages\n",
      "\n",
      "• Customize the settings using your favorite email applications for better support\n",
      "\n",
      "• Support for Google Inbox One and Oneplus: Support for this product requires Adobe\n"
     ]
    }
   ],
   "source": [
    "numreps = 10 # num of random repetitions\n",
    "numtoks = 100 # oputput length\n",
    "\n",
    "# we need model to generate 1000 toks, but the output should be meaningul\n",
    "# if we ask to generate 1000 toks in a single seq, the model goes awkard\n",
    "# so idea is to have more short seq than fewer long seq\n",
    "\n",
    "# random starting tokens\n",
    "randstarts = torch.randint(tokenizer.vocab_size, (numreps,1)).to(device) # this creates a 10x1 matrix [10 btaches of one single starting token]\n",
    "\n",
    "out = gpt2.generate(\n",
    "    randstarts,\n",
    "    max_length  = numtoks+1, #the first token is the row start in randstarts, so you need 100+1 total toks in output of generate()\n",
    "    min_length = numtoks+1, # guarantee that model should generarte exact;y 100 toks\n",
    "    do_sample = True,\n",
    "    bad_words_ids = [tokenizer.encode(tokenizer.eos_token)],\n",
    "    pad_token_id = tokenizer.encode(tokenizer.eos_token)[0]).cpu()\n",
    "\n",
    "print(out,'\\n')\n",
    "\n",
    "for o in out:\n",
    "    print('\\n*** Next batch ofg outoput')\n",
    "    print(tokenizer.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89fc4541-1148-4750-8c7b-fc89c190411e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18987],\n",
       "        [43375],\n",
       "        [22983],\n",
       "        [21365],\n",
       "        [33019],\n",
       "        [32545],\n",
       "        [46765],\n",
       "        [ 8515],\n",
       "        [27956],\n",
       "        [17620]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randstarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a94b781-67cf-4b6d-9056-124a9746a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gullivers travels common tokens appeared in 26.3% of new tokens\n"
     ]
    }
   ],
   "source": [
    "# calcualte and report the percentage\n",
    "percentFreqTokens_pre = np.mean(100*np.isin(out[:,1:],top100).flatten())\n",
    "print(f\"Gullivers travels common tokens appeared in {percentFreqTokens_pre}% of new tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c06996-ea0e-4b5d-ae1c-dd0cc6cf0df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0189582-19dc-4149-a738-f4b0b3adcb2b",
   "metadata": {},
   "source": [
    "Fine tune the model (to imporve the above percent, i.e to include more GT text in model outout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bf1c050-9804-467a-bc9d-370aaacf2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt2.parameters(), lr=5e-5, weight_decay=.01) # here the learning rate is really small\n",
    "\n",
    "#NOTE: IMP dont need loss func here, bcoz HF models calcualte loss fun internally \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d9b9053-55a9-479b-b424-f2dec990c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0/1234, train loss: 2.242738962173462\n",
      "Sample 77/1234, train loss: 0.09334482252597809\n",
      "Sample 154/1234, train loss: 0.04268364980816841\n",
      "Sample 231/1234, train loss: 0.03527035936713219\n",
      "Sample 308/1234, train loss: 0.030546387657523155\n",
      "Sample 385/1234, train loss: 0.03709830343723297\n",
      "Sample 462/1234, train loss: 0.028301220387220383\n",
      "Sample 539/1234, train loss: 0.024902261793613434\n",
      "Sample 616/1234, train loss: 0.029406629502773285\n",
      "Sample 693/1234, train loss: 0.027280081063508987\n",
      "Sample 770/1234, train loss: 0.0290165226906538\n",
      "Sample 847/1234, train loss: 0.02049478515982628\n",
      "Sample 924/1234, train loss: 0.022254755720496178\n",
      "Sample 1001/1234, train loss: 0.021956220269203186\n",
      "Sample 1078/1234, train loss: 0.0224310215562582\n",
      "Sample 1155/1234, train loss: 0.026207711547613144\n",
      "Sample 1232/1234, train loss: 0.016983982175588608\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1234\n",
    "\n",
    "#init the loss\n",
    "train_loss = np.zeros(num_samples)\n",
    "\n",
    "\n",
    "for sampli in range(num_samples):\n",
    "    ix = torch.randint(len(gtTokens)-seq_len,size=(batch_size,))\n",
    "    X = gtTokens[ix[:,None]+ torch.arange(seq_len)]\n",
    "    X = X.to(device)\n",
    "    gpt2.zero_grad()\n",
    "    # fwd pass (HF shifts X internally to get y)\n",
    "    # all of X is shifted by 1 and used as labels for loss calcualtion\n",
    "    # also inside model, H.F makes sure it uses NLLLoss as loss func\n",
    "    # fwd pass\n",
    "    outputs= gpt2(X,labels=X)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # sum the batch loss\n",
    "    train_loss[sampli] = loss.item()\n",
    "    if sampli%77==0:\n",
    "        print(f'Sample {sampli}/{num_samples}, train loss: {train_loss[sampli]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77b8b909-ecbe-42d0-a2c3-758c09970326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot believe that a website would think this.\"\n",
      "\n",
      "\"Maybe you think so. Don't you think? Don't you think that the website you are trying to reach has some kind of broken link you think may have occurred to you. Go to <a href=\"/\">www.gutenberg.org</a> to see whether the error persists.</p>\n",
      "\n",
      "<p>If you think something is broken, follow the âContact Informationâ�\n"
     ]
    }
   ],
   "source": [
    "prompt = 'I cannot believe that'\n",
    "in2gpt = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "output = gpt2.generate(in2gpt, max_length=100, pad_token_id=50256,do_sample=True).cpu()\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e879d28d-736f-4f44-9db0-50a0f0c38a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common GT tokens usage went from 26.30% to 26.30% after fine tuning\n"
     ]
    }
   ],
   "source": [
    "# calcualte and report the percentage\n",
    "percentFreqTokens_pst = np.mean(100*np.isin(out[:,1:],top100).flatten())\n",
    "print(f\"Common GT tokens usage went from {percentFreqTokens_pre:.2f}% to {percentFreqTokens_pst:.2f}% after fine tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c1c30-d402-4460-8633-8847d003b8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
