{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab85bd05-48d3-414d-a275-5ed172f5279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raeez/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69dab816-3690-4eba-851d-9acf354d4f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆ| 148/148 [00:00<00:00, 1886.36it/s, Materializing param=\n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18030529-e306-437f-b39c-5f00cee9f650",
   "metadata": {},
   "source": [
    "Pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf8817b-b6b8-429b-86a9-23ea26f04120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pad token: None\n",
      "Current eos token: <|endoftext|>\n",
      "Current pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# GPT2 doesnot have pad token\n",
    "print('Current pad token:', tokenizer.pad_token)\n",
    "print('Current eos token:', tokenizer.eos_token)\n",
    "\n",
    "# common (thought not necessarly ideal) to set pad_token = eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print('Current pad token:', tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2412c-b179-48d1-8414-d3b9390413c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c4455e4-88db-43b0-ba74-34d91b6f36b8",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9150de3-68ee-4ffa-950f-104060f4aca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 6109,  3772,  1641,   318,   262,   976, 50256, 50256, 50256, 50256],\n",
       "        [16833, 19283,  1641,   318, 19283,   287,   663,   898,   835,    13],\n",
       "        [ 5195,   750,   262,  9015,  3272,   262,  2975,    30, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'Every happy family is the same',\n",
    "    'every unhappy family is unhappy in its own way.',\n",
    "    'Why did the chicken cross the road?']\n",
    "# these sentences have diff no of tokens\n",
    "\n",
    "# tokenize the padding and attention mask\n",
    "toks = tokenizer(sentences,\n",
    "                 return_tensors='pt',\n",
    "                 padding=True) #this is important\n",
    "toks\n",
    "#50256 padding tokens (only because we tied it to EOS token 50256 is EOS token actually\n",
    "\n",
    "# also look into the attention masks for 1st and 2nd sentence where it was padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acafb10a-650f-4e0c-bfed-a9214c2dc277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8945f8f9-e17c-4446-b9cd-eb15ca94cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Every happy family is the same\":\n",
      "tensor([ 6109,  3772,  1641,   318,   262,   976, 50256, 50256, 50256, 50256]) \n",
      " tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0]) \n",
      "\n",
      "\"every unhappy family is unhappy in its own way.\":\n",
      "tensor([16833, 19283,  1641,   318, 19283,   287,   663,   898,   835,    13]) \n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) \n",
      "\n",
      "\"Why did the chicken cross the road?\":\n",
      "tensor([ 5195,   750,   262,  9015,  3272,   262,  2975,    30, 50256, 50256]) \n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    inputs = toks['input_ids'][i]\n",
    "    mask = toks['attention_mask'][i]\n",
    "    print(f'\"{sentences[i]}\":')\n",
    "    print(inputs,'\\n',mask,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebe1de-ae5e-4cc8-82b4-8ab27f9195ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d70ea761-c409-4224-b4c5-5bc4994148e4",
   "metadata": {},
   "source": [
    "generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c22a9c0-5828-48ff-8cbc-c7fd4ba726ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6109,  3772,  1641,   318,   262,   976, 50256, 50256, 50256, 50256,\n",
       "           198,   198,    40,   561,   910,   345,   821,   826,   986,   314,\n",
       "          1101,  7926,   986,   475,   340,   338,   691,   262,   835,   356,\n",
       "          2107,    13,   314,  1101,  7926,    13,   314,  1101,  7926,    13,\n",
       "           314,  1053,   587,  7926,   329,  1115,   812,    13,   314,  1053,\n",
       "           587,  7926,   329,  1115,  7028,    13,   843,   314,  1101,   655,\n",
       "          7926,   546,   340,    11,   290,   314],\n",
       "        [16833, 19283,  1641,   318, 19283,   287,   663,   898,   835,    13,\n",
       "           632,   318,   477,   262,   976,   284,   606,    13,  1081,   890,\n",
       "           355,   262,  1641,   389, 19283,    11,   262, 19283,  1641,   318,\n",
       "           691, 19283,   611,   340,   318, 19283,   287,   663,   898,   835,\n",
       "            13,  1649,   484,   765,   617,   584,  1611,   286, 12157,    11,\n",
       "           484,   910,    11,   366,  2949,    11,   286,  1781,   407,   526,\n",
       "           383, 49414,  1641,   318, 49414,   611],\n",
       "        [ 5195,   750,   262,  9015,  3272,   262,  2975,    30, 50256, 50256,\n",
       "          1858,   318,   281,  7468,    25,   257,  1588,  1271,   286,  6844,\n",
       "          3272,   262, 12763,    13,   632,   318,   262,   749,  2219,  3272,\n",
       "            12,  4679,   276,   287,  8372,  3284,    13,   554,  1109,    11,\n",
       "           287,  3945,   286,   428,   614,    11,   257,  2008,  4120,  4478,\n",
       "           262,  3290, 12538,   257,  7696,   422,   810,   257,  7696,   373,\n",
       "          4385,   284,   307,  3170,    11,   290]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids = toks['input_ids'],\n",
    "    attention_mask = toks['attention_mask'],\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    max_length = 66,\n",
    "    num_return_sequences=1,\n",
    "    do_sample =True,\n",
    "    top_k = 50,\n",
    "    top_p = .95)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2129d8b2-f06c-49d9-ba27-3a235eb4916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "923cf7b8-2581-45ca-90e3-c946ca4d5652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 66])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a14f957-1030-49ec-bdbd-ee048cf82cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[6109, 3772, 1641,  318,  262,  976]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "Out: tensor([[6109, 3772, 1641,  318,  262,  976,   13],\n",
      "        [6109, 3772, 1641,  318,  262,  976,  355]])\n"
     ]
    }
   ],
   "source": [
    "s = 'Every happy family is the same'\n",
    "t = tokenizer(s,return_tensors='pt')\n",
    "print(t)\n",
    "o = model.generate(\n",
    "    input_ids = t['input_ids'],\n",
    "    attention_mask = t['attention_mask'],\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    max_length = 7,\n",
    "    num_return_sequences=2,\n",
    "    do_sample =True,\n",
    "    top_k = 50,\n",
    "    top_p = .95)\n",
    "print('Out:',o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148eda7b-be87-4120-8948-b89c3e326a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a87c4d79-9362-4497-8270-7c95ca5893b4",
   "metadata": {},
   "source": [
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d44df698-1833-474a-951d-e8e9d36f4492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Text 1: \n",
      "Every happy family is the same\n",
      "\n",
      "I would say you're right... I'm sorry... but it's only the way we live. I'm sorry. I'm sorry. I've been sorry for three years. I've been sorry for three seasons. And I'm just sorry about it, and I\n",
      "\n",
      "** Text 2: \n",
      "every unhappy family is unhappy in its own way. It is all the same to them. As long as the family are unhappy, the unhappy family is only unhappy if it is unhappy in its own way. When they want some other kind of happiness, they say, \"No, of course not.\" The happiest family is happiest if\n",
      "\n",
      "** Text 3: \n",
      "Why did the chicken cross the road?There is an explanation: a large number of dogs cross the highway. It is the most common cross-breed in southern Russia. In fact, in February of this year, a video appeared showing the dog crossing a bridge from where a bridge was supposed to be built, and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for i , text in enumerate(decoded_texts):\n",
    "    print(f'** Text {i+1}: \\n{text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b4cda52-cdd3-40b6-98d6-57d5a3dd1ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# or simplerL\n",
    "os = model.generate(toks['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88b4cd20-35f3-4723-b0f2-072d226c46f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bd91b7f-d61f-44d5-afd5-a3157c617f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6109,  3772,  1641,   318,   262,   976, 50256, 50256, 50256, 50256,\n",
       "           464,   471,    13,    50,    13,  2732,   286,  4796,   468,  4884,\n",
       "           257, 33262,   284,   262,  1664,   326, 12216,   262,  2351,  4765],\n",
       "        [16833, 19283,  1641,   318, 19283,   287,   663,   898,   835,    13,\n",
       "           198,   198,     1,    40,   892,   340,   338,   257,   845,  1593,\n",
       "          2071,   329,   262,  1499,    13,   632,   338,   257,   845,  1593],\n",
       "        [ 5195,   750,   262,  9015,  3272,   262,  2975,    30, 50256, 50256,\n",
       "           464,   471,  1546,    47, 32603,   784,  3406,  2723,   329,   383,\n",
       "         15624, 28859,  1201,  8735,   198,   198,  1212,  2708,   318,   546]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddeabce-b5a1-43d1-a3f8-5783f96aa903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
