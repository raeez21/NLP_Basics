{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fb616a-4753-4f04-af67-d78555fecfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181334cc-bbf7-4379-ba2d-5a93243d26c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,000 tokens in vocab and 70 embedding dimensions\n"
     ]
    }
   ],
   "source": [
    "# create the 2 tensor objects\n",
    "vocab_size = 5000\n",
    "embed_dim = int(vocab_size**(1/2))\n",
    "print(f'{vocab_size:,} tokens in vocab and {embed_dim} embedding dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b5a3b6-43e1-451f-a34d-883b5eed9be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(5000, 70)\n",
      "Linear(in_features=70, out_features=5000, bias=True)\n",
      "\n",
      "Embedding matrix size: torch.Size([5000, 70])\n",
      "Linear matreix size: torch.Size([5000, 70])\n"
     ]
    }
   ],
   "source": [
    "E = nn.Embedding(vocab_size, embed_dim) #input [in X out]\n",
    "L = nn.Linear(embed_dim, vocab_size)  #input [out X in]\n",
    "\n",
    "print(E)\n",
    "print(L)\n",
    "print(f'\\nEmbedding matrix size: {E.weight.shape}')\n",
    "print(f'Linear matreix size: {L.weight.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcca463-45cd-47f3-b9b4-08886ab494d1",
   "metadata": {},
   "source": [
    "\\explopre their attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c631eb-5e39-43f7-8d19-9bd2a48d2d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 123)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dir(E)), len(dir(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fee985-3b1e-4714-97ea-9fd4f7ccca5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_fill_padding_idx_with_zero',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embedding_dim',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'max_norm',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm_type',\n",
       " 'num_embeddings',\n",
       " 'padding_idx',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'scale_grad_by_freq',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'sparse',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c2a1c0-5aa1-48b8-b986-c3c24783e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrributes only in Embedding\n",
      " _fill_padding_idx_with_zero\n",
      " embedding_dim\n",
      " from_pretrained\n",
      " max_norm\n",
      " norm_type\n",
      " num_embeddings\n",
      " padding_idx\n",
      " scale_grad_by_freq\n",
      " sparse\n",
      "\n",
      "\n",
      "Attrributes only in Linear\n",
      " bias\n",
      " in_features\n",
      " out_features\n"
     ]
    }
   ],
   "source": [
    "# unique attritbues of each type\n",
    "attrE = dir(E)\n",
    "attrL = dir(L)\n",
    "\n",
    "print('Attrributes only in Embedding')\n",
    "for e in attrE:\n",
    "    if e not in attrL:\n",
    "        print(' ' +e)\n",
    "\n",
    "\n",
    "print('\\n\\nAttrributes only in Linear')\n",
    "for l in attrL:\n",
    "    if l not in attrE:\n",
    "        print(' ' +l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ee9c3-9f4b-453f-aec5-d8160ec7e2ab",
   "metadata": {},
   "source": [
    "indexing their vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ea3d2f-3503-4675-aaa2-42c6c9c94c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3875, -0.2554, -0.0241,  ..., -0.2724, -1.0058,  0.4520],\n",
      "        [-1.4386,  1.2612, -0.5140,  ...,  0.3780, -0.2661,  1.3075],\n",
      "        [-0.1758,  1.8739,  1.3123,  ...,  0.2986,  0.3940,  0.6581],\n",
      "        ...,\n",
      "        [ 0.0237,  1.0179, -1.2630,  ...,  0.5935,  0.5056, -0.4228],\n",
      "        [-0.1758,  0.1083, -0.5631,  ...,  0.3422,  1.2250,  1.2936],\n",
      "        [-0.4512, -0.7831,  0.9035,  ...,  0.0901,  1.2556, -0.3130]],\n",
      "       requires_grad=True)\n",
      " \n",
      "Parameter containing:\n",
      "tensor([[-0.0502, -0.0671,  0.0977,  ..., -0.0917,  0.0670, -0.0776],\n",
      "        [ 0.1112, -0.0927,  0.0520,  ...,  0.0824, -0.1127, -0.1019],\n",
      "        [ 0.0199, -0.0337,  0.0737,  ..., -0.0910,  0.0409,  0.0645],\n",
      "        ...,\n",
      "        [-0.0014, -0.0551,  0.1041,  ...,  0.0976,  0.0914,  0.1014],\n",
      "        [ 0.0077, -0.0776, -0.0100,  ..., -0.0181, -0.0176, -0.0862],\n",
      "        [-0.1156, -0.0038,  0.0038,  ..., -0.0970, -0.0852,  0.1179]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(E.weight)\n",
    "print(' ')\n",
    "print(L.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1724814d-8bc4-451e-8a85-9abf92d92fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4386,  1.2612, -0.5140,  0.9556, -0.2304,  0.5485, -0.0558,  0.5411,\n",
       "         -0.6446,  1.0935,  0.0613,  0.0670, -1.5341, -1.2291, -0.0214, -0.1050,\n",
       "         -0.6782, -0.2176, -0.5108, -0.6156, -0.0923, -0.0560,  0.7473, -0.5905,\n",
       "          0.6720, -0.2263,  0.0920, -1.7169,  0.1925, -0.9999, -1.2995, -0.4284,\n",
       "          0.2939, -1.6564,  1.3641,  1.3400,  0.5402,  0.5719,  0.2240, -1.1140,\n",
       "          0.6492,  2.3213,  0.0476, -0.2344, -1.0540,  1.3522,  1.0682, -0.5033,\n",
       "         -0.3513, -0.0350, -1.5617, -0.2588, -0.5635,  0.1417, -0.1068,  1.2529,\n",
       "         -1.2172, -1.6056,  0.6365,  0.4061,  0.5729, -2.0690, -1.5862, -0.3745,\n",
       "         -1.1023,  0.8391,  0.3190,  0.3780, -0.2661,  1.3075]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenidx = torch.tensor([1])\n",
    "E(tokenidx) # emulaters one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38daf803-42d5-4cd7-84e2-b5e47dd98bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenidx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#this wont work\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/jupyter-env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "L(tokenidx)\n",
    "#this wont work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344e7d54-d8c3-4bfa-9ac4-1b55c425f294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1112, -0.0927,  0.0520,  0.1153, -0.0635,  0.0643, -0.0502, -0.1075,\n",
       "          0.0204, -0.0824, -0.0919,  0.0604,  0.0892, -0.0041, -0.0757, -0.0743,\n",
       "          0.0442,  0.1074,  0.1150, -0.0764, -0.0490,  0.0204, -0.1076,  0.0363,\n",
       "         -0.0140,  0.0540, -0.0854,  0.0154, -0.0139,  0.0440,  0.0952,  0.0080,\n",
       "          0.0452,  0.0612, -0.0989, -0.0886,  0.1187, -0.0585, -0.1144, -0.0870,\n",
       "          0.0712,  0.0134, -0.0959, -0.0432,  0.1128,  0.0076,  0.0513,  0.0970,\n",
       "         -0.0009,  0.0130, -0.0607, -0.0584,  0.0051,  0.0772,  0.0824, -0.0608,\n",
       "         -0.1052,  0.0653,  0.0388,  0.1161, -0.0729, -0.0343,  0.0830,  0.0981,\n",
       "         -0.0156,  0.0267,  0.0120,  0.0824, -0.1127, -0.1019]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  use this\n",
    "L.weight[tokenidx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3dfd387-681c-429d-a9ac-015e9dc2ec1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(vocab_size,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa96234-3e19-46da-ae4d-ea0defb819a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
