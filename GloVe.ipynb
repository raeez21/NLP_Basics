{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99c751b-4a7a-4346-b639-cd0bd9d2c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02633dd-f989-4e31-af46-1e0b5554862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a sample corpus to build a vocab\n",
    "texts = ['text', 'the', 'leader', 'prime', 'natural', 'languages','leader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad276fa-5837-456a-b2e3-7be722ee5148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary = 6\n",
      "Dictionary is = {'leader': 1, 'text': 2, 'the': 3, 'prime': 4, 'natural': 5, 'languages': 6}\n"
     ]
    }
   ],
   "source": [
    "# Initialise and fit tokenizer\n",
    "# prints num of unique words and their assigned indices\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Output the word-index dictionary\n",
    "print(\"Number of unique words in dictionary =\", len(tokenizer.word_index))\n",
    "print(\"Dictionary is =\", tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76034387-c88c-4f0e-82ec-0f222f48113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to Create embedding matrix\n",
    "# loads GloVe word vectors from file.\n",
    "# Creates an embedding matrix matching tokenizer word indices with GloVe vectors.\n",
    "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # +1 for padding token (index 0) index of word_idnex starts at 1\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding ='utf8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix_vocab           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "296a4446-6cdc-42ec-ae1b-591646f45490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download the GloVe dataset\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "\n",
    "# Unzip the file\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd62de4-a67a-41cf-a3e3-e324b7032aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
